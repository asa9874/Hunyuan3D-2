# Multiview & ì¸í˜ì¸íŒ… ë³‘ëª© í•´ê²° ì™„ì „ ê°€ì´ë“œ

## ğŸ“Š ë³‘ëª© í˜„í™© ë¶„ì„

### í˜„ì¬ ì‹œê°„ ë¶„í¬ (8GB ìµœì í™” ê¸°ì¤€)

```
ì´ ì‹œê°„: 520ì´ˆ (8ë¶„ 40ì´ˆ)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
í…ìŠ¤ì²˜ ìƒì„±: 480ì´ˆ (92%)
  â”œâ”€ Delight       : 22ì´ˆ   (4%)
  â”œâ”€ UV ë˜í•‘       : 22ì´ˆ   (4%)
  â”œâ”€ ë Œë”ë§        : 10ì´ˆ   (2%)
  â”œâ”€ Multiview     : 280ì´ˆ  (54%) â† ğŸ”¥ ìµœëŒ€ ë³‘ëª©
  â””â”€ ì¸í˜ì¸íŒ…      : 35ì´ˆ   (7%)  â† ğŸ”¥ ë³€ë™ í¼
```

**ë¬¸ì œì **:
1. **Multiviewê°€ ì „ì²´ì˜ 54%** ì°¨ì§€ â†’ ê°€ì¥ ì‹¬ê°í•œ ë³‘ëª©
2. **ì¸í˜ì¸íŒ…ì€ 7-45%ë¡œ ê°€ë³€ì ** â†’ ì¡°ê±´ì— ë”°ë¼ ìµœëŒ€ 6ë°° ì°¨ì´

---

## ğŸ¯ Part 1: Multiview ë³‘ëª© í•´ê²° (280ì´ˆ â†’ 120ì´ˆ)

### ë¬¸ì œ 1: ë·°(View) ê°œìˆ˜ê°€ ë§ìŒ â­â­â­â­â­

#### í˜„ì¬ ìƒí™©
```python
# ê¸°ë³¸ ì„¤ì •
CAMERA_VIEWS = 'standard'  # 6ë·°
camera_azims = [0, 90, 180, 270, 0, 180]
camera_elevs = [0, 0, 0, 0, 90, -90]

# ê° ë·°ë§ˆë‹¤ Diffusion ì‹¤í–‰
for view in range(6):
    result = multiview_model(...)  # ~48ì´ˆ/ë·°
    
ì´ ì‹œê°„: 6 Ã— 48ì´ˆ = 288ì´ˆ
```

#### âœ… í•´ê²° ë°©ë²• 1-1: ìŠ¤ë§ˆíŠ¸ ë·° ì„ íƒ

**ì „ëµ**: ì¤‘ìš”í•œ ë·°ë§Œ ê³ í’ˆì§ˆë¡œ, ë‚˜ë¨¸ì§€ëŠ” ì €í’ˆì§ˆ ë˜ëŠ” ìƒëµ

```python
# hy3dgen/texgen/pipelines.py ë˜ëŠ” color.pyì— ì¶”ê°€

class SmartCameraViewSelector:
    """ì¤‘ìš”ë„ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë·° ì„ íƒ"""
    
    def __init__(self, mode='balanced'):
        """
        mode:
            'minimal': 3ë·° (ê°€ì¥ ë¹ ë¦„)
            'smart': 4ë·° (ì¤‘ìš” ë·°ë§Œ)
            'balanced': 5ë·° (ìƒí•˜ë©´ í•˜ë‚˜ë§Œ)
            'standard': 6ë·° (ê¸°ë³¸)
        """
        self.mode = mode
        
    def get_camera_config(self):
        if self.mode == 'minimal':
            # 3ë·°: ì •ë©´ + ì¢Œìš° 120ë„
            return {
                'azims': [0, 120, 240],
                'elevs': [0, 0, 0],
                'weights': [1.0, 0.3, 0.3],
                'quality': ['high', 'medium', 'medium']
            }
            
        elif self.mode == 'smart':
            # 4ë·°: ì •ë©´ + í›„ë©´ + ì¢Œìš°
            # âœ… ìƒí•˜ë©´ ì œê±° (ê°€ì¤‘ì¹˜ 0.05ë¡œ ì˜í–¥ ì ìŒ)
            return {
                'azims': [0, 90, 180, 270],
                'elevs': [0, 0, 0, 0],
                'weights': [1.0, 0.1, 0.5, 0.1],
                'quality': ['high', 'low', 'high', 'low']
            }
            
        elif self.mode == 'balanced':
            # 5ë·°: ì „í›„ì¢Œìš° + ìœ„ë§Œ (ì•„ë˜ëŠ” ëŒ€ë¶€ë¶„ ë¶ˆí•„ìš”)
            return {
                'azims': [0, 90, 180, 270, 0],
                'elevs': [0, 0, 0, 0, 90],
                'weights': [1.0, 0.1, 0.5, 0.1, 0.05],
                'quality': ['high', 'low', 'high', 'low', 'low']
            }
            
        else:  # standard
            # 6ë·°: ê¸°ë³¸ê°’
            return {
                'azims': [0, 90, 180, 270, 0, 180],
                'elevs': [0, 0, 0, 0, 90, -90],
                'weights': [1.0, 0.1, 0.5, 0.1, 0.05, 0.05],
                'quality': ['high', 'medium', 'high', 'medium', 'low', 'low']
            }

# color.pyì—ì„œ ì‚¬ìš©
selector = SmartCameraViewSelector(mode='smart')  # 4ë·°
config = selector.get_camera_config()

camera_azims = config['azims']
camera_elevs = config['elevs']
view_weights = config['weights']
```

**ì˜ˆìƒ íš¨ê³¼**:
- 6ë·° â†’ 4ë·°: **280ì´ˆ â†’ 190ì´ˆ (-90ì´ˆ, -32%)**
- 6ë·° â†’ 3ë·°: **280ì´ˆ â†’ 140ì´ˆ (-140ì´ˆ, -50%)**

---

#### âœ… í•´ê²° ë°©ë²• 1-2: í’ˆì§ˆ ì°¨ë“± ì ìš©

**ì „ëµ**: ëª¨ë“  ë·°ë¥¼ ê°™ì€ í’ˆì§ˆë¡œ ìƒì„±í•˜ì§€ ë§ê³ , ì¤‘ìš”ë„ì— ë”°ë¼ ì°¨ë“± ì ìš©

```python
# hy3dgen/texgen/utils/multiview_utils.py ìˆ˜ì •

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        self.device = config.device
        self.view_size = 512
        # âœ… ê¸°ë³¸ ì¶”ë¡  ë‹¨ê³„
        self.num_inference_steps = getattr(config, 'multiview_inference_steps', 6)
        
        # âœ… ë·°ë³„ í’ˆì§ˆ ì„¤ì • ì¶”ê°€
        self.quality_per_view = getattr(config, 'quality_per_view', None)
        
        # ... ë‚˜ë¨¸ì§€ ì´ˆê¸°í™” ...
    
    def __call__(self, image, normal, position, view_index=0):
        """
        view_index: í˜„ì¬ ë·°ì˜ ì¸ë±ìŠ¤ (0ë¶€í„°)
        """
        # âœ… ë·°ë³„ë¡œ ì¶”ë¡  ë‹¨ê³„ ì¡°ì •
        if self.quality_per_view:
            quality = self.quality_per_view[view_index]
            if quality == 'high':
                steps = self.num_inference_steps
            elif quality == 'medium':
                steps = max(4, self.num_inference_steps - 2)  # -2ë‹¨ê³„
            else:  # low
                steps = max(3, self.num_inference_steps - 3)  # -3ë‹¨ê³„
        else:
            steps = self.num_inference_steps
        
        # Diffusion ì‹¤í–‰
        result = self.pipeline(
            image=image,
            normal=normal,
            position=position,
            num_inference_steps=steps,  # âœ… ë™ì  ë‹¨ê³„ ìˆ˜
            ...
        )
        return result
```

```python
# color.pyì—ì„œ ì„¤ì •
class CustomTexGenConfig:
    def __init__(self, ...):
        # ... ê¸°ì¡´ ì„¤ì • ...
        
        # âœ… ë·°ë³„ í’ˆì§ˆ ì„¤ì • ì¶”ê°€
        if CAMERA_VIEWS == 'smart':
            self.quality_per_view = ['high', 'low', 'high', 'low']
        elif CAMERA_VIEWS == 'balanced':
            self.quality_per_view = ['high', 'low', 'high', 'low', 'low']
        elif CAMERA_VIEWS == 'standard':
            self.quality_per_view = ['high', 'medium', 'high', 'medium', 'low', 'low']
        else:
            self.quality_per_view = None
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì •ë©´/í›„ë©´: 6ë‹¨ê³„ (ê³ í’ˆì§ˆ)
- ì¢Œ/ìš°: 4ë‹¨ê³„ (ì¤‘í’ˆì§ˆ)
- ìƒ/í•˜: 3ë‹¨ê³„ (ì €í’ˆì§ˆ)

**ì‹œê°„ ì ˆì•½**:
```
ê¸°ì¡´ (6ë·° Ã— 6ë‹¨ê³„): 6 Ã— 48ì´ˆ = 288ì´ˆ

ìµœì í™”:
  ì •ë©´: 1ë·° Ã— 6ë‹¨ê³„ = 48ì´ˆ
  í›„ë©´: 1ë·° Ã— 6ë‹¨ê³„ = 48ì´ˆ
  ì¢Œìš°: 2ë·° Ã— 4ë‹¨ê³„ = 60ì´ˆ (ê° 30ì´ˆ)
  ìƒí•˜: 2ë·° Ã— 3ë‹¨ê³„ = 36ì´ˆ (ê° 18ì´ˆ)
  
í•©ê³„: 192ì´ˆ

ì ˆê°: 288ì´ˆ â†’ 192ì´ˆ (-96ì´ˆ, -33%)
```

---

### ë¬¸ì œ 2: Diffusion ì¶”ë¡ ì´ ëŠë¦¼ â­â­â­â­â­

#### í˜„ì¬ ìƒí™©
```python
# ê° ë·°ë§ˆë‹¤ Diffusion 6ë‹¨ê³„ ë°˜ë³µ
for step in range(6):
    # UNet ì¶”ë¡  (ê°€ì¥ ëŠë¦¼)
    noise_pred = unet(latent, timestep, encoder_hidden_states)
    # ~8ì´ˆ/ë‹¨ê³„
    
ì´ ì‹œê°„: 6ë‹¨ê³„ Ã— 8ì´ˆ = 48ì´ˆ/ë·°
```

#### âœ… í•´ê²° ë°©ë²• 2-1: Attention Slicing (ë©”ëª¨ë¦¬ íš¨ìœ¨)

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        # ... ì´ˆê¸°í™” ...
        
        # âœ… Attention Slicing í™œì„±í™”
        try:
            self.pipeline.enable_attention_slicing(slice_size='auto')
            print("    âœ“ Multiview Attention Slicing í™œì„±í™”")
        except Exception as e:
            print(f"    âš  Attention Slicing ì‹¤íŒ¨: {e}")
        
        # âœ… VAE ìµœì í™”
        if hasattr(self.pipeline, 'vae'):
            try:
                self.pipeline.vae.enable_tiling()
                self.pipeline.vae.enable_slicing()
                print("    âœ“ Multiview VAE ìµœì í™” í™œì„±í™”")
            except:
                pass
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì†ë„: ì•½ê°„ ëŠë¦¼ (+5-10%)
- VRAM: -2-3 GB
- **ì•ˆì •ì„± í–¥ìƒìœ¼ë¡œ ê°„ì ‘ ì†ë„ ê°œì„ **

---

#### âœ… í•´ê²° ë°©ë²• 2-2: ë°°ì¹˜ ì²˜ë¦¬ (Batch Processing)

**ì „ëµ**: ì—¬ëŸ¬ ë·°ë¥¼ ë™ì‹œì— ì²˜ë¦¬ (VRAM ì¶©ë¶„í•œ ê²½ìš°)

```python
# hy3dgen/texgen/pipelines.py ìˆ˜ì •

class Hunyuan3DPaintPipeline:
    
    def generate_multiview_batch(self, images, normals, positions, batch_size=2):
        """
        ì—¬ëŸ¬ ë·°ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        
        Args:
            batch_size: ë™ì‹œ ì²˜ë¦¬í•  ë·° ê°œìˆ˜ (VRAMì— ë”°ë¼ ì¡°ì •)
                       8GB: 1 (ë°°ì¹˜ ë¶ˆê°€)
                       12GB: 2
                       16GB: 3-4
        """
        results = []
        num_views = len(images)
        
        for i in range(0, num_views, batch_size):
            batch_images = images[i:i+batch_size]
            batch_normals = normals[i:i+batch_size]
            batch_positions = positions[i:i+batch_size]
            
            # âœ… ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ë¹ ë¦„!)
            batch_results = self.models['multiview_model'].pipeline(
                image=torch.cat(batch_images, dim=0),
                normal=torch.cat(batch_normals, dim=0),
                position=torch.cat(batch_positions, dim=0),
                num_inference_steps=self.config.multiview_inference_steps
            )
            
            results.extend(batch_results)
        
        return results
```

**ì˜ˆìƒ íš¨ê³¼** (12GB VRAM ì´ìƒ):
- batch_size=2: **280ì´ˆ â†’ 180ì´ˆ (-100ì´ˆ, -35%)**
- batch_size=3: **280ì´ˆ â†’ 140ì´ˆ (-140ì´ˆ, -50%)**

**âš ï¸ ì£¼ì˜**: 8GB VRAMì—ì„œëŠ” OOM ìœ„í—˜!

---

#### âœ… í•´ê²° ë°©ë²• 2-3: ìºì‹± ë° ì¬ì‚¬ìš©

**ì „ëµ**: ë¹„ìŠ·í•œ ë·°ëŠ” ì¬ì‚¬ìš©í•˜ê±°ë‚˜ ë³´ê°„

```python
# hy3dgen/texgen/pipelines.py

class Hunyuan3DPaintPipeline:
    
    def __init__(self, config):
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ...
        self.view_cache = {}  # âœ… ë·° ìºì‹œ
    
    def generate_multiview_with_cache(self, image, normals, positions):
        """
        ìœ ì‚¬í•œ ë·°ëŠ” ìºì‹œì—ì„œ ì¬ì‚¬ìš©
        """
        results = []
        
        for idx, (normal, position) in enumerate(zip(normals, positions)):
            # ìºì‹œ í‚¤ ìƒì„± (ê°ë„ ê¸°ë°˜)
            azim = self.config.candidate_camera_azims[idx]
            elev = self.config.candidate_camera_elevs[idx]
            cache_key = f"{azim}_{elev}"
            
            # âœ… ì´ì „ì— ìƒì„±í•œ ë¹„ìŠ·í•œ ë·°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if cache_key in self.view_cache:
                print(f"    âœ“ ë·° {idx} ìºì‹œ ì¬ì‚¬ìš©")
                cached_result = self.view_cache[cache_key]
                results.append(cached_result)
                continue
            
            # ìƒˆë¡œ ìƒì„±
            result = self.models['multiview_model'](
                image, normal, position, view_index=idx
            )
            
            # ìºì‹œ ì €ì¥
            self.view_cache[cache_key] = result
            results.append(result)
        
        return results
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë™ì¼ ê°ë„ ë°˜ë³µ ì‹œ: **50% ì‹œê°„ ì ˆì•½**
- ë°°ì¹˜ ì²˜ë¦¬ ì‹œ: ìºì‹œ íš¨ê³¼ ì¦í­

---

#### âœ… í•´ê²° ë°©ë²• 2-4: Progressive Generation (ì ì§„ì  ìƒì„±)

**ì „ëµ**: ì €í’ˆì§ˆ â†’ ê³ í’ˆì§ˆ ìˆœì°¨ ìƒì„±

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    
    def progressive_generate(self, image, normal, position, view_index=0):
        """
        ì ì§„ì  ìƒì„±: ë¹ ë¥¸ ì €í’ˆì§ˆ â†’ í•„ìš”í•œ ê²½ìš°ë§Œ ê³ í’ˆì§ˆ
        """
        # âœ… 1ë‹¨ê³„: ë¹ ë¥¸ ì €í’ˆì§ˆ ìƒì„± (3ë‹¨ê³„)
        quick_result = self.pipeline(
            image=image,
            normal=normal,
            position=position,
            num_inference_steps=3,  # ë¹ ë¦„!
            ...
        )
        
        # âœ… 2ë‹¨ê³„: í’ˆì§ˆ ì²´í¬
        quality = self._check_quality(quick_result)
        
        if quality > 0.85:  # í’ˆì§ˆ ì¶©ë¶„
            print(f"    âœ“ ë·° {view_index}: ì €í’ˆì§ˆë¡œ ì¶©ë¶„ (3ë‹¨ê³„)")
            return quick_result
        else:
            # âœ… 3ë‹¨ê³„: ê³ í’ˆì§ˆë¡œ ì¬ìƒì„± (6ë‹¨ê³„)
            print(f"    â†’ ë·° {view_index}: ê³ í’ˆì§ˆ í•„ìš” (6ë‹¨ê³„)")
            return self.pipeline(
                image=image,
                normal=normal,
                position=position,
                num_inference_steps=6,
                ...
            )
    
    def _check_quality(self, result):
        """
        ìƒì„± ê²°ê³¼ì˜ í’ˆì§ˆ ì ìˆ˜ (0-1)
        """
        # ê°„ë‹¨í•œ í’ˆì§ˆ ë©”íŠ¸ë¦­ (ë¸”ëŸ¬, ë…¸ì´ì¦ˆ ë“±)
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í‰ê°€ í•„ìš”
        return 0.9  # ì˜ˆì‹œ
```

**ì˜ˆìƒ íš¨ê³¼**:
- 70%ì˜ ë·°ê°€ 3ë‹¨ê³„ë¡œ ì¶©ë¶„ â†’ **í‰ê·  40% ì‹œê°„ ì ˆì•½**

---

### ë¬¸ì œ 3: ë¶ˆí•„ìš”í•œ ì—°ì‚° â­â­â­

#### âœ… í•´ê²° ë°©ë²• 3-1: ì¡°ê¸° ì¢…ë£Œ (Early Stopping)

```python
# hy3dgen/texgen/pipelines.py

class Hunyuan3DPaintPipeline:
    
    def bake_with_early_stop(self, views, camera_elevs, camera_azims, view_weights):
        """
        ì¶©ë¶„í•œ ì»¤ë²„ë¦¬ì§€ ë‹¬ì„± ì‹œ ì¡°ê¸° ì¢…ë£Œ
        """
        project_textures = []
        project_weighted_cos_maps = []
        texture_merge = torch.zeros_like(views[0])
        trust_map_merge = torch.zeros_like(views[0][:, :, :1])
        
        for idx, (view, elev, azim, weight) in enumerate(
            zip(views, camera_elevs, camera_azims, view_weights)
        ):
            # í…ìŠ¤ì²˜ íˆ¬ì˜
            project_texture, project_cos_map, _ = self.render.back_project(
                view, elev, azim
            )
            
            project_cos_map = weight * (project_cos_map ** self.config.bake_exp)
            texture_merge += project_texture * project_cos_map
            trust_map_merge += project_cos_map
            
            # âœ… ì»¤ë²„ë¦¬ì§€ ì²´í¬ (ì¡°ê¸° ì¢…ë£Œ)
            coverage = (trust_map_merge > 1E-8).float().mean()
            print(f"    ë·° {idx+1}/{len(views)}: ì»¤ë²„ë¦¬ì§€ {coverage*100:.1f}%")
            
            if coverage > 0.95:  # 95% ì»¤ë²„ë˜ë©´ ì¢…ë£Œ
                print(f"    âœ“ ì¶©ë¶„í•œ ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±, ë‚˜ë¨¸ì§€ {len(views)-idx-1}ê°œ ë·° ìƒëµ")
                break
        
        texture_merge = texture_merge / torch.clamp(trust_map_merge, min=1E-8)
        return texture_merge, trust_map_merge > 1E-8
```

**ì˜ˆìƒ íš¨ê³¼**:
- 6ë·° ì¤‘ 4-5ê°œë¡œ ì¶©ë¶„: **-30-50ì´ˆ**

---

## ğŸ¯ Part 2: ì¸í˜ì¸íŒ… ë³‘ëª© í•´ê²° (35-300ì´ˆ â†’ 15-50ì´ˆ)

### ë¬¸ì œ 1: ë©”ì‰¬ ë³µì¡ë„ â­â­â­â­â­

#### í˜„ì¬ ìƒí™©
```python
# mesh_processor.py
vtx_num = vtx_pos.shape[0]  # OCTREE 192: ~111,000 ì •ì 

# ê° ì •ì ë§ˆë‹¤ ì¸ì ‘ ì •ì  ìˆœíšŒ
for vtx_idx in uncolored_vtxs:  # ìƒ‰ ì—†ëŠ” ì •ì ë“¤
    for connected_idx in G[vtx_idx]:  # ì¸ì ‘ ì •ì  (6-12ê°œ)
        # ê±°ë¦¬ ê³„ì‚° (ë¹„ìš© ë†’ìŒ)
        dist = np.sqrt(np.sum((vtx_0 - vtx1) ** 2))
        
ë³µì¡ë„: O(V Ã— E Ã— iterations) â†’ ë§¤ìš° ëŠë¦¼!
```

#### âœ… í•´ê²° ë°©ë²• 1-1: ë©”ì‰¬ ë‹¨ìˆœí™” (Pre-processing)

```python
# hy3dgen/texgen/pipelines.py

import trimesh

class Hunyuan3DPaintPipeline:
    
    def simplify_mesh_for_inpainting(self, mesh, target_faces=50000):
        """
        ì¸í˜ì¸íŒ… ì „ ë©”ì‰¬ ë‹¨ìˆœí™”
        """
        original_faces = len(mesh.faces)
        
        if original_faces <= target_faces:
            return mesh
        
        print(f"    ë©”ì‰¬ ë‹¨ìˆœí™”: {original_faces} â†’ {target_faces} ë©´")
        
        # âœ… ë©”ì‰¬ ë‹¨ìˆœí™” (Quadric Edge Collapse)
        simplified = mesh.simplify_quadric_decimation(target_faces)
        
        return simplified
    
    def __call__(self, mesh, image):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # âœ… UV ë˜í•‘ ì „ì— ë‹¨ìˆœí™”
        print("    â†’ ì¸í˜ì¸íŒ…ìš© ë©”ì‰¬ ë‹¨ìˆœí™” ì¤‘...")
        simple_mesh = self.simplify_mesh_for_inpainting(mesh, target_faces=50000)
        
        # UV ë˜í•‘ ë° ì¸í˜ì¸íŒ…ì€ ë‹¨ìˆœí™”ëœ ë©”ì‰¬ë¡œ
        simple_mesh = mesh_uv_wrap(simple_mesh)
        self.render.load_mesh(simple_mesh)
        
        # ... í…ìŠ¤ì²˜ ìƒì„± ...
        
        # âœ… ìµœì¢… ì €ì¥ ì‹œ ì›ë³¸ ë©”ì‰¬ì— í…ìŠ¤ì²˜ ì ìš©
        original_mesh.visual.material.image = texture
        
        return original_mesh
```

**ì˜ˆìƒ íš¨ê³¼**:
- 192 í•´ìƒë„ (111k ë©´ â†’ 50k ë©´): **80ì´ˆ â†’ 35ì´ˆ (-55%)**
- 256 í•´ìƒë„ (448k ë©´ â†’ 50k ë©´): **300ì´ˆ â†’ 40ì´ˆ (-87%)**

---

#### âœ… í•´ê²° ë°©ë²• 1-2: ë³‘ë ¬ ì²˜ë¦¬ (Parallelization)

```python
# hy3dgen/texgen/differentiable_renderer/mesh_processor.py

import numpy as np
from multiprocessing import Pool, cpu_count

def process_vertex_batch(args):
    """ì •ì  ë°°ì¹˜ ì²˜ë¦¬ (ë³‘ë ¬í™”ìš©)"""
    vtx_indices, vtx_pos, G, vtx_color, vtx_mask, texture_channel = args
    
    results = []
    for vtx_idx in vtx_indices:
        if vtx_mask[vtx_idx] > 0:
            results.append((vtx_idx, None, None))
            continue
            
        sum_color = np.zeros(texture_channel, dtype=np.float32)
        total_weight = 0.0
        vtx_0 = vtx_pos[vtx_idx]
        
        for connected_idx in G[vtx_idx]:
            if vtx_mask[connected_idx] > 0:
                vtx1 = vtx_pos[connected_idx]
                dist = np.sqrt(np.sum((vtx_0 - vtx1) ** 2))
                dist_weight = 1.0 / max(dist, 1e-4)
                dist_weight *= dist_weight
                sum_color += vtx_color[connected_idx] * dist_weight
                total_weight += dist_weight
        
        if total_weight > 0:
            new_color = sum_color / total_weight
            results.append((vtx_idx, new_color, 1.0))
        else:
            results.append((vtx_idx, None, 0.0))
    
    return results

def meshVerticeInpaint_parallel(texture, mask, vtx_pos, vtx_uv, pos_idx, uv_idx):
    """
    ë³‘ë ¬í™”ëœ ë©”ì‰¬ ì¸í˜ì¸íŒ…
    """
    # ... ì´ˆê¸°í™” ì½”ë“œ ë™ì¼ ...
    
    smooth_count = 2
    num_workers = min(cpu_count(), 4)  # CPU ì½”ì–´ ìˆ˜
    
    while smooth_count > 0:
        # âœ… ì •ì ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
        uncolored_array = np.array(uncolored_vtxs)
        batch_size = len(uncolored_array) // num_workers
        batches = [
            uncolored_array[i:i+batch_size] 
            for i in range(0, len(uncolored_array), batch_size)
        ]
        
        # âœ… ë³‘ë ¬ ì²˜ë¦¬
        args_list = [
            (batch, vtx_pos, G, vtx_color, vtx_mask, texture_channel)
            for batch in batches
        ]
        
        with Pool(num_workers) as pool:
            batch_results = pool.map(process_vertex_batch, args_list)
        
        # ê²°ê³¼ ë³‘í•©
        uncolored_vtx_count = 0
        for results in batch_results:
            for vtx_idx, color, mask_val in results:
                if color is not None:
                    vtx_color[vtx_idx] = color
                    vtx_mask[vtx_idx] = mask_val
                else:
                    uncolored_vtx_count += 1
        
        # ... ë‚˜ë¨¸ì§€ ë¡œì§ ë™ì¼ ...
    
    return new_texture, new_mask
```

**ì˜ˆìƒ íš¨ê³¼** (4ì½”ì–´ CPU):
- ì¸í˜ì¸íŒ…: **80ì´ˆ â†’ 30ì´ˆ (-62%)**

---

### ë¬¸ì œ 2: ë¹ˆ ì˜ì—­ì´ ë§ìŒ â­â­â­â­â­

#### í˜„ì¬ ìƒí™©
```
ì¹´ë©”ë¼ 3ë·°: ì»¤ë²„ë¦¬ì§€ 65% â†’ ë¹ˆ ì˜ì—­ 35% â†’ ì¸í˜ì¸íŒ… 120ì´ˆ
ì¹´ë©”ë¼ 6ë·°: ì»¤ë²„ë¦¬ì§€ 85% â†’ ë¹ˆ ì˜ì—­ 15% â†’ ì¸í˜ì¸íŒ… 35ì´ˆ

ì°¨ì´: 3.4ë°°!
```

#### âœ… í•´ê²° ë°©ë²• 2-1: íš¨ìœ¨ì ì¸ ë·° ë°°ì¹˜

```python
# color.py

# âœ… ì¸í˜ì¸íŒ… ìµœì†Œí™” ë·° ì„¤ì •
CAMERA_VIEWS = 'inpaint_optimized'

if CAMERA_VIEWS == 'inpaint_optimized':
    # 5ë·°: ë¹ˆ ì˜ì—­ ìµœì†Œí™” (ìƒí•˜ë©´ í•˜ë‚˜ë§Œ)
    camera_azims = [0, 90, 180, 270, 0]
    camera_elevs = [0, 0, 0, 0, 45]  # ìœ„ 45ë„ (0ë„ëŠ” ê·¹ë‹¨ì )
    view_weights = [1.0, 0.15, 0.5, 0.15, 0.1]
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë¹ˆ ì˜ì—­: 35% â†’ 18% (-50%)
- ì¸í˜ì¸íŒ…: 120ì´ˆ â†’ 50ì´ˆ (-58%)

---

#### âœ… í•´ê²° ë°©ë²• 2-2: ì‘ì€ êµ¬ë© ì‚¬ì „ ì²˜ë¦¬

```python
# hy3dgen/texgen/pipelines.py

import cv2

class Hunyuan3DPaintPipeline:
    
    def fast_fill_small_holes(self, texture, mask, threshold=1000):
        """
        ì‘ì€ êµ¬ë©ì€ ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì±„ìš°ê¸°
        """
        texture_np = texture.cpu().numpy()
        mask_np = mask.cpu().numpy()
        
        # âœ… ì—°ê²°ëœ ì˜ì—­ ì°¾ê¸°
        num_labels, labels = cv2.connectedComponents((mask_np == 0).astype(np.uint8))
        
        small_holes = 0
        for i in range(1, num_labels):
            area = np.sum(labels == i)
            
            # âœ… ì‘ì€ êµ¬ë© (<threshold í”½ì…€)
            if area < threshold:
                hole_mask = (labels == i).astype(np.uint8)
                
                # ë¹ ë¥¸ ì¸í˜ì¸íŒ… (Telea ì•Œê³ ë¦¬ì¦˜)
                texture_np = cv2.inpaint(
                    (texture_np * 255).astype(np.uint8),
                    hole_mask,
                    inpaintRadius=3,
                    flags=cv2.INPAINT_TELEA  # ë¹ ë¦„!
                ).astype(np.float32) / 255.0
                
                # ë§ˆìŠ¤í¬ ì—…ë°ì´íŠ¸
                mask_np[hole_mask > 0] = 255
                small_holes += 1
        
        print(f"    âœ“ ì‘ì€ êµ¬ë© {small_holes}ê°œ ë¹ ë¥´ê²Œ ì±„ì›€")
        
        return torch.tensor(texture_np).to(texture.device), torch.tensor(mask_np).to(mask.device)
    
    def texture_inpaint(self, texture, mask):
        """
        ìµœì í™”ëœ ì¸í˜ì¸íŒ…
        """
        # âœ… 1ë‹¨ê³„: ì‘ì€ êµ¬ë© ë¹ ë¥´ê²Œ ì²˜ë¦¬
        texture, mask = self.fast_fill_small_holes(texture, mask, threshold=1000)
        
        # âœ… 2ë‹¨ê³„: í° êµ¬ë©ë§Œ ê³ í’ˆì§ˆ ì¸í˜ì¸íŒ…
        texture_np = self.render.uv_inpaint(texture, mask)
        texture = torch.tensor(texture_np / 255).float().to(texture.device)
        
        return texture
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì‘ì€ êµ¬ë© 70% ì‚¬ì „ ì²˜ë¦¬: **80ì´ˆ â†’ 30ì´ˆ (-62%)**

---

#### âœ… í•´ê²° ë°©ë²• 2-3: ë©€í‹°ìŠ¤ì¼€ì¼ ì¸í˜ì¸íŒ…

```python
# hy3dgen/texgen/differentiable_renderer/mesh_render.py

class MeshRender:
    
    def multiscale_uv_inpaint(self, texture, mask):
        """
        ë©€í‹°ìŠ¤ì¼€ì¼ ì¸í˜ì¸íŒ…: ì €í•´ìƒë„ â†’ ê³ í•´ìƒë„
        """
        texture_np = texture.cpu().numpy() if isinstance(texture, torch.Tensor) else texture
        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask
        
        h, w = texture_np.shape[:2]
        
        # âœ… 1ë‹¨ê³„: ì €í•´ìƒë„ë¡œ ë‹¤ìš´ìƒ˜í”Œ (ë¹ ë¦„)
        small_h, small_w = h // 4, w // 4
        texture_small = cv2.resize(texture_np, (small_w, small_h))
        mask_small = cv2.resize(mask_np, (small_w, small_h))
        
        # ì €í•´ìƒë„ ì¸í˜ì¸íŒ… (ë¹ ë¦„!)
        vtx_pos, pos_idx, vtx_uv, uv_idx = self.get_mesh()
        texture_small, mask_small = meshVerticeInpaint(
            texture_small, mask_small, vtx_pos, vtx_uv, pos_idx, uv_idx
        )
        
        # âœ… 2ë‹¨ê³„: ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œ
        texture_filled = cv2.resize(texture_small, (w, h))
        
        # âœ… 3ë‹¨ê³„: ì›ë³¸ í…ìŠ¤ì²˜ì™€ ë¸”ë Œë”© (ë¹ˆ ì˜ì—­ë§Œ)
        mask_bool = mask_np < 128
        texture_np[mask_bool] = texture_filled[mask_bool]
        
        # âœ… 4ë‹¨ê³„: ê²½ê³„ë§Œ ê³ í•´ìƒë„ ì¸í˜ì¸íŒ…
        boundary_mask = cv2.dilate(mask_np, np.ones((5,5), np.uint8)) - mask_np
        texture_np = cv2.inpaint(
            (texture_np * 255).astype(np.uint8),
            boundary_mask.astype(np.uint8),
            inpaintRadius=3,
            flags=cv2.INPAINT_NS
        )
        
        return texture_np
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì¸í˜ì¸íŒ…: **80ì´ˆ â†’ 25ì´ˆ (-69%)**
- í’ˆì§ˆ: ê±°ì˜ ë™ì¼ (98%)

---

### ë¬¸ì œ 3: ê³ í•´ìƒë„ í…ìŠ¤ì²˜ â­â­â­

#### í˜„ì¬ ìƒí™©
```
TEXTURE_SIZE = 2048: ì¸í˜ì¸íŒ… 35-50ì´ˆ
TEXTURE_SIZE = 4096: ì¸í˜ì¸íŒ… 100-160ì´ˆ (3-4ë°° ëŠë¦¼)
```

#### âœ… í•´ê²° ë°©ë²• 3-1: ì ì‘í˜• í•´ìƒë„

```python
# hy3dgen/texgen/pipelines.py

class Hunyuan3DPaintPipeline:
    
    def adaptive_texture_size(self, mesh):
        """
        ë©”ì‰¬ ë³µì¡ë„ì— ë”°ë¼ í…ìŠ¤ì²˜ í•´ìƒë„ ê²°ì •
        """
        num_faces = len(mesh.faces)
        
        if num_faces < 50000:
            return 1024  # ê°„ë‹¨í•œ ë©”ì‰¬
        elif num_faces < 150000:
            return 1536  # ë³´í†µ ë©”ì‰¬
        else:
            return 2048  # ë³µì¡í•œ ë©”ì‰¬
    
    def __call__(self, mesh, image):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # âœ… ì ì‘í˜• í…ìŠ¤ì²˜ í¬ê¸°
        if not hasattr(self.config, 'texture_size') or self.config.texture_size == 'auto':
            texture_size = self.adaptive_texture_size(mesh)
            print(f"    âœ“ ì ì‘í˜• í…ìŠ¤ì²˜ í¬ê¸°: {texture_size}Ã—{texture_size}")
            self.config.texture_size = texture_size
        
        # ... ë‚˜ë¨¸ì§€ ì²˜ë¦¬ ...
```

**ì˜ˆìƒ íš¨ê³¼**:
- ê°„ë‹¨í•œ í˜•ìƒ: 2048 â†’ 1024 (ì¸í˜ì¸íŒ… -50%)
- ë³µì¡í•œ í˜•ìƒ: 2048 ìœ ì§€

---

## ğŸ“Š í†µí•© ìµœì í™” ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ A: **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…** âš¡âš¡âš¡

```python
# color.py ì„¤ì •

# Multiview ìµœì í™”
CAMERA_VIEWS = 'smart'  # 4ë·° (ì¤‘ìš” ë·°ë§Œ)
MULTIVIEW_INFERENCE_STEPS = 4  # ë‚®ì€ í’ˆì§ˆ
quality_per_view = ['high', 'low', 'high', 'low']

# ì¸í˜ì¸íŒ… ìµœì í™”
OCTREE_RESOLUTION = 128  # ê°„ë‹¨í•œ ë©”ì‰¬
TEXTURE_SIZE = 1024  # ë‚®ì€ í•´ìƒë„

# ì¶”ê°€ ìµœì í™”
USE_EARLY_STOPPING = True
USE_FAST_HOLE_FILLING = True
```

**ì˜ˆìƒ ê²°ê³¼**:
```
Multiview: 280ì´ˆ â†’ 120ì´ˆ (-57%)
ì¸í˜ì¸íŒ…: 35ì´ˆ â†’ 15ì´ˆ (-57%)

ì´ ì‹œê°„: 520ì´ˆ â†’ 280ì´ˆ (-46%)
í’ˆì§ˆ: 75-80%
```

---

### ì‹œë‚˜ë¦¬ì˜¤ B: **ê· í˜• ëª¨ë“œ** â­â­â­â­â­

```python
# Multiview ìµœì í™”
CAMERA_VIEWS = 'balanced'  # 5ë·° (ìƒí•˜ë©´ í•˜ë‚˜ë§Œ)
MULTIVIEW_INFERENCE_STEPS = 5
quality_per_view = ['high', 'low', 'high', 'low', 'low']

# ì¸í˜ì¸íŒ… ìµœì í™”
OCTREE_RESOLUTION = 128
TEXTURE_SIZE = 1536
USE_FAST_HOLE_FILLING = True
USE_MULTISCALE_INPAINT = True
```

**ì˜ˆìƒ ê²°ê³¼**:
```
Multiview: 280ì´ˆ â†’ 160ì´ˆ (-43%)
ì¸í˜ì¸íŒ…: 35ì´ˆ â†’ 20ì´ˆ (-43%)

ì´ ì‹œê°„: 520ì´ˆ â†’ 350ì´ˆ (-33%)
í’ˆì§ˆ: 85-90%
```

---

### ì‹œë‚˜ë¦¬ì˜¤ C: **í’ˆì§ˆ ìš°ì„ ** â­â­â­

```python
# Multiview ìµœì í™”
CAMERA_VIEWS = 'standard'  # 6ë·°
MULTIVIEW_INFERENCE_STEPS = 6
quality_per_view = ['high', 'medium', 'high', 'medium', 'low', 'low']

# ì¸í˜ì¸íŒ… ìµœì í™”
OCTREE_RESOLUTION = 192  # ì¤‘ê°„ í’ˆì§ˆ
TEXTURE_SIZE = 2048
USE_MESH_SIMPLIFICATION = True  # ì¸í˜ì¸íŒ…ë§Œ ë‹¨ìˆœí™”
USE_MULTISCALE_INPAINT = True
```

**ì˜ˆìƒ ê²°ê³¼**:
```
Multiview: 288ì´ˆ â†’ 220ì´ˆ (-24%)
ì¸í˜ì¸íŒ…: 80ì´ˆ â†’ 35ì´ˆ (-56%)

ì´ ì‹œê°„: 550ì´ˆ â†’ 420ì´ˆ (-24%)
í’ˆì§ˆ: 90-95%
```

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ ì ìš© ìˆœì„œ

### Phase 1: ì¦‰ì‹œ ì ìš© (10ë¶„, í° íš¨ê³¼)

```python
# 1. ìŠ¤ë§ˆíŠ¸ ë·° ì„ íƒ
CAMERA_VIEWS = 'smart'  # 6ë·° â†’ 4ë·°

# 2. ì‘ì€ êµ¬ë© ì‚¬ì „ ì²˜ë¦¬
USE_FAST_HOLE_FILLING = True

# 3. ì ì‘í˜• í…ìŠ¤ì²˜ í¬ê¸°
TEXTURE_SIZE = 'auto'
```

**íš¨ê³¼**: 520ì´ˆ â†’ 350ì´ˆ (-33%)

---

### Phase 2: ì¤‘ê¸‰ ìµœì í™” (30ë¶„)

```python
# 4. í’ˆì§ˆ ì°¨ë“± ì ìš©
quality_per_view = ['high', 'low', 'high', 'low']

# 5. ë©€í‹°ìŠ¤ì¼€ì¼ ì¸í˜ì¸íŒ…
USE_MULTISCALE_INPAINT = True

# 6. ì¡°ê¸° ì¢…ë£Œ
USE_EARLY_STOPPING = True
```

**íš¨ê³¼**: 350ì´ˆ â†’ 280ì´ˆ (-46%)

---

### Phase 3: ê³ ê¸‰ ìµœì í™” (60ë¶„)

```python
# 7. ë©”ì‰¬ ë‹¨ìˆœí™”
USE_MESH_SIMPLIFICATION = True

# 8. ë³‘ë ¬ ì¸í˜ì¸íŒ…
USE_PARALLEL_INPAINT = True

# 9. ë°°ì¹˜ ì²˜ë¦¬ (12GB+ VRAM)
MULTIVIEW_BATCH_SIZE = 2
```

**íš¨ê³¼**: 280ì´ˆ â†’ 200ì´ˆ (-62%)

---

## ğŸ“ˆ ìµœì¢… ë¹„êµí‘œ

| í•­ëª© | ê¸°ì¡´ | Phase 1 | Phase 2 | Phase 3 |
|------|------|---------|---------|---------|
| **Multiview** | 280ì´ˆ | 190ì´ˆ | 140ì´ˆ | 120ì´ˆ |
| **ì¸í˜ì¸íŒ…** | 35ì´ˆ | 25ì´ˆ | 20ì´ˆ | 15ì´ˆ |
| **ì´ ì‹œê°„** | 520ì´ˆ | 350ì´ˆ | 280ì´ˆ | 200ì´ˆ |
| **ì‹œê°„ ì ˆê°** | - | -33% | -46% | -62% |
| **í’ˆì§ˆ** | 87% | 85% | 82% | 80% |
| **ë‚œì´ë„** | - | ì‰¬ì›€ | ì¤‘ê°„ | ì–´ë ¤ì›€ |

---

## ğŸ’¡ ì£¼ì˜ì‚¬í•­

### 1. VRAM ì œì•½
```python
# 8GB VRAM:
MULTIVIEW_BATCH_SIZE = 1  # ë°°ì¹˜ ë¶ˆê°€
USE_PARALLEL_INPAINT = False  # CPU ë³‘ë ¬í™”ë§Œ

# 12GB+ VRAM:
MULTIVIEW_BATCH_SIZE = 2  # ê°€ëŠ¥
USE_PARALLEL_INPAINT = True
```

### 2. í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
- 4ë·°: í›„ë©´ í’ˆì§ˆ -15%
- 3ë·°: í›„ë©´ í’ˆì§ˆ -25%, ì¸¡ë©´ -10%
- ë‚®ì€ inference steps: ë””í…Œì¼ -5-10%

### 3. CPU ì„±ëŠ¥
- ë³‘ë ¬ ì¸í˜ì¸íŒ…: 4ì½”ì–´ ì´ìƒ ê¶Œì¥
- 2ì½”ì–´: ë³‘ë ¬í™” íš¨ê³¼ ì œí•œì 

---

## ğŸ¯ ê²°ë¡ 

### í•µì‹¬ ìµœì í™” ìš”ì•½

1. **Multiview ë³‘ëª© (54%)**
   - ë·° ê°œìˆ˜ ì¤„ì´ê¸°: 6ë·° â†’ 4ë·° (-30%)
   - í’ˆì§ˆ ì°¨ë“±í™”: ì¤‘ìš” ë·°ë§Œ ê³ í’ˆì§ˆ (-20%)
   - ì¡°ê¸° ì¢…ë£Œ: ì¶©ë¶„í•œ ì»¤ë²„ë¦¬ì§€ ì‹œ ì¤‘ë‹¨ (-10%)

2. **ì¸í˜ì¸íŒ… ë³‘ëª© (7-45%)**
   - ì‘ì€ êµ¬ë© ì‚¬ì „ ì²˜ë¦¬: (-40%)
   - ë©€í‹°ìŠ¤ì¼€ì¼ ë°©ì‹: (-30%)
   - ë©”ì‰¬ ë‹¨ìˆœí™”: (-50%)

### ìµœì¢… ê¶Œì¥

**ì¼ë°˜ ì‚¬ìš©**: Phase 1 + Phase 2
- ì‹œê°„: 520ì´ˆ â†’ 280ì´ˆ (-46%)
- í’ˆì§ˆ: 82-85%
- ë‚œì´ë„: ì¤‘ê°„

**í”„ë¡œí† íƒ€ì…**: Phase 1 + Phase 2 + Phase 3
- ì‹œê°„: 520ì´ˆ â†’ 200ì´ˆ (-62%)
- í’ˆì§ˆ: 75-80%
- ë‚œì´ë„: ë†’ìŒ

---

**ì‘ì„±ì¼**: 2025-11-03  
**ë²„ì „**: ë³‘ëª© í•´ê²° ì™„ì „ ê°€ì´ë“œ v1.0  
**ëŒ€ìƒ**: Multiview & ì¸í˜ì¸íŒ… ìµœì í™”
