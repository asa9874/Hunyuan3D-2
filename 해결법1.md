# í…ìŠ¤ì²˜ ìƒì„± ë³‘ëª© í˜„ìƒ ìµœì í™” ë°©ì•ˆ

## ğŸ“Š í˜„ì¬ ë³‘ëª© êµ¬ê°„ ë¶„ì„

í”Œë¡œìš°.mdì— ë”°ë¥´ë©´ í…ìŠ¤ì²˜ ìƒì„± ë‹¨ê³„ì—ì„œ ì£¼ìš” ë³‘ëª©ì€:

1. **Multiview ëª¨ë¸** (36.8-74.0%) - ìµœëŒ€ ë³‘ëª©
2. **í…ìŠ¤ì²˜ ì¸í˜ì¸íŒ…** (7.3-45.6%) - ë³µì¡ë„ì— ë”°ë¼ ê°€ë³€
3. **UV ë˜í•‘** (5.2-11.8%) - ë©”ì‰¬ í•´ìƒë„ì— ë¹„ë¡€
4. **Delight ëª¨ë¸** (1.9-4.4%)

---

## ğŸš€ ì½”ë“œ ë ˆë²¨ ìµœì í™” ë°©ì•ˆ

### 1. **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”** â­â­â­

#### ë¬¸ì œì 
í˜„ì¬ `multiview_utils.py`ì˜ `Multiview_Diffusion_Net`ëŠ” ê° ë·°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

#### í•´ê²°ì±…: GPU ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬

```python
# hy3dgen/texgen/utils/multiview_utils.py ìˆ˜ì •

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        self.device = config.device
        self.view_size = 512
        self.num_inference_steps = getattr(config, 'multiview_inference_steps', 6)
        self.batch_size = getattr(config, 'batch_size', 2)  # âœ… ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°
        multiview_ckpt_path = config.multiview_ckpt_path

        current_file_path = os.path.abspath(__file__)
        custom_pipeline_path = os.path.join(os.path.dirname(current_file_path), '..', 'hunyuanpaint')

        pipeline = DiffusionPipeline.from_pretrained(
            multiview_ckpt_path,
            custom_pipeline=custom_pipeline_path, 
            torch_dtype=torch.float16
        )

        if config.pipe_name in ['hunyuanpaint']:
            pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(
                pipeline.scheduler.config,
                timestep_spacing='trailing'
            )
        elif config.pipe_name in ['hunyuanpaint-turbo']:
            pipeline.scheduler = LCMScheduler.from_config(
                pipeline.scheduler.config,
                timestep_spacing='trailing'
            )
            pipeline.set_turbo(True)

        # âœ… VAE íƒ€ì¼ë§ í™œì„±í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        if hasattr(pipeline, 'vae'):
            pipeline.vae.enable_tiling()
            pipeline.vae.enable_slicing()

        self.pipeline = pipeline.to(self.device)

    def __call__(self, input_images, control_images, camera_info):
        self.seed_everything(0)

        if not isinstance(input_images, List):
            input_images = [input_images]

        input_images = [img.resize((self.view_size, self.view_size)) for img in input_images]
        
        for i in range(len(control_images)):
            control_images[i] = control_images[i].resize((self.view_size, self.view_size))
            if control_images[i].mode == 'L':
                control_images[i] = control_images[i].point(lambda x: 255 if x > 1 else 0, mode='1')

        num_view = len(control_images) // 2
        
        # âœ… ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        all_images = []
        for batch_start in range(0, num_view, self.batch_size):
            batch_end = min(batch_start + self.batch_size, num_view)
            batch_indices = list(range(batch_start, batch_end))
            
            normal_image = [[control_images[i] for i in batch_indices]]
            position_image = [[control_images[i + num_view] for i in batch_indices]]
            camera_info_gen = [[camera_info[i] for i in batch_indices]]
            camera_info_ref = [[0]]

            kwargs = {
                'generator': torch.Generator(device=self.pipeline.device).manual_seed(0),
                'width': self.view_size,
                'height': self.view_size,
                'num_in_batch': len(batch_indices),
                'camera_info_gen': camera_info_gen,
                'camera_info_ref': camera_info_ref,
                'normal_imgs': normal_image,
                'position_imgs': position_image
            }

            batch_images = self.pipeline(
                input_images, 
                num_inference_steps=self.num_inference_steps, 
                **kwargs
            ).images
            all_images.extend(batch_images)

        return all_images
```

**color.pyì—ì„œ ì‚¬ìš©ë²•:**
```python
class CustomTexGenConfig:
    def __init__(self, light_remover_ckpt_path, multiview_ckpt_path, subfolder_name):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        self.batch_size = 2  # âœ… GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • (1-3)
```

**ì˜ˆìƒ íš¨ê³¼:**
- 6ë·° ì²˜ë¦¬ ì‹œ: ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ **30-40% ì†ë„ í–¥ìƒ**
- ë©”ëª¨ë¦¬ ì—¬ìœ  ìˆìœ¼ë©´ `batch_size=3`ìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ

---

### 2. **ëª¨ë¸ ìµœì í™”: Torch Compile** â­â­â­

#### í•´ê²°ì±…: PyTorch 2.0ì˜ ì»´íŒŒì¼ ìµœì í™”

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ ...
        
        self.pipeline = pipeline.to(self.device)
        
        # âœ… Torch Compileë¡œ UNet ìµœì í™”
        if hasattr(torch, 'compile'):
            try:
                print("    â†’ UNet ì»´íŒŒì¼ ì¤‘ (ìµœì´ˆ 1íšŒë§Œ ëŠë¦¼)...")
                self.pipeline.unet = torch.compile(
                    self.pipeline.unet,
                    mode='reduce-overhead',  # 'default', 'reduce-overhead', 'max-autotune'
                    fullgraph=False
                )
                print("    âœ“ UNet ì»´íŒŒì¼ ì™„ë£Œ!")
            except Exception as e:
                print(f"    âš  ì»´íŒŒì¼ ì‹¤íŒ¨ (ì¼ë°˜ ëª¨ë“œë¡œ ê³„ì†): {e}")
```

```python
# hy3dgen/texgen/utils/dehighlight_utils.py

class Light_Shadow_Remover():
    def __init__(self, config):
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ì½”ë“œ ...
        
        self.pipeline = pipeline.to(self.device, torch.float16)
        
        # âœ… Torch Compileë¡œ UNet ìµœì í™”
        if hasattr(torch, 'compile'):
            try:
                print("    â†’ Delight UNet ì»´íŒŒì¼ ì¤‘...")
                self.pipeline.unet = torch.compile(
                    self.pipeline.unet,
                    mode='reduce-overhead'
                )
                print("    âœ“ ì»´íŒŒì¼ ì™„ë£Œ!")
            except Exception as e:
                print(f"    âš  ì»´íŒŒì¼ ì‹¤íŒ¨: {e}")
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì²« ì‹¤í–‰: +30ì´ˆ (ì»´íŒŒì¼ ì‹œê°„)
- ì´í›„ ì‹¤í–‰: **20-30% ì†ë„ í–¥ìƒ**
- Multiview: ~40ì´ˆ â†’ ~28ì´ˆ/ë·°
- Delight: ~18ì´ˆ â†’ ~12ì´ˆ

---

### 3. **Mixed Precision ìµœì í™” (ìë™ ìºìŠ¤íŒ…)** â­â­

#### í•´ê²°ì±…: torch.autocast í™œìš©

```python
# hy3dgen/texgen/pipelines.pyì˜ __call__ ë©”ì†Œë“œ ìˆ˜ì •

@torch.no_grad()
def __call__(self, mesh, image):
    import time
    profiling = {}
    total_start = time.time()

    if not isinstance(image, List):
        image = [image]

    # ... ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì½”ë“œ ...

    # âœ… Autocastë¡œ ê°ì‹¸ê¸°
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):
        # 2. Delight ëª¨ë¸
        step_start = time.time()
        print("    â†’ [2/11] Delight ëª¨ë¸ ì‹¤í–‰ ì¤‘ (ê·¸ë¦¼ì/í•˜ì´ë¼ì´íŠ¸ ì œê±°)...")
        images_prompt = [self.models['delight_model'](image_prompt) for image_prompt in images_prompt]
        profiling['2_delight_model'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['2_delight_model']:.2f}ì´ˆ")

    # 3-6ì€ ê·¸ëŒ€ë¡œ (ë Œë”ë§ì€ autocast ë¶ˆí•„ìš”)
    
    with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):
        # 7. Multiview ìƒì„±
        step_start = time.time()
        print("    â†’ [7/11] Multiview ëª¨ë¸ ì‹¤í–‰ ì¤‘ (ë©€í‹°ë·° ì´ë¯¸ì§€ ìƒì„±)...")
        camera_info = [(((azim // 30) + 9) % 12) // {-20: 1, 0: 1, 20: 1, -90: 3, 90: 3}[
            elev] + {-20: 0, 0: 12, 20: 24, -90: 36, 90: 40}[elev] for azim, elev in
                       zip(selected_camera_azims, selected_camera_elevs)]
        multiviews = self.models['multiview_model'](images_prompt, normal_maps + position_maps, camera_info)
        profiling['7_multiview_model'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['7_multiview_model']:.2f}ì´ˆ")

    # ë‚˜ë¨¸ì§€ ë‹¨ê³„ë“¤...
```

**ì˜ˆìƒ íš¨ê³¼:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: **10-15% ê°ì†Œ**
- ì†ë„: **5-10% í–¥ìƒ**

---

### 4. **ë Œë”ë§ ë³‘ë ¬í™”** â­â­

#### í˜„ì¬ ë¬¸ì œ
Normal/Position ë§µ ë Œë”ë§ì´ ìˆœì°¨ ì²˜ë¦¬ë©ë‹ˆë‹¤.

```python
# í˜„ì¬ ì½”ë“œ (pipelines.py)
normal_maps = self.render_normal_multiview(
    selected_camera_elevs, selected_camera_azims, use_abs_coor=True)
position_maps = self.render_position_multiview(
    selected_camera_elevs, selected_camera_azims)
```

#### í•´ê²°ì±…: ë™ì‹œ ë Œë”ë§

```python
# hy3dgen/texgen/pipelines.pyì— ìƒˆ ë©”ì†Œë“œ ì¶”ê°€

def render_normal_and_position_multiview(self, camera_elevs, camera_azims):
    """Normalê³¼ Positionì„ ë™ì‹œì— ë Œë”ë§"""
    normal_maps = []
    position_maps = []
    
    for elev, azim in zip(camera_elevs, camera_azims):
        # âœ… í•œ ë²ˆì˜ ë Œë”ë§ìœ¼ë¡œ ë‘ ë§µ ìƒì„±
        normal_map = self.render.render_normal(
            elev, azim, use_abs_coor=True, return_type='pl')
        position_map = self.render.render_position(
            elev, azim, return_type='pl')
        
        normal_maps.append(normal_map)
        position_maps.append(position_map)
    
    return normal_maps, position_maps

# __call__ ë©”ì†Œë“œì—ì„œ ì‚¬ìš©
def __call__(self, mesh, image):
    # ...
    
    # 5-6ë‹¨ê³„ í†µí•©
    step_start = time.time()
    print(f"    â†’ [5/11] Normal & Position ë§µ ë Œë”ë§ ì¤‘ ({len(selected_camera_elevs)}ê°œ ë·°)...")
    normal_maps, position_maps = self.render_normal_and_position_multiview(
        selected_camera_elevs, selected_camera_azims)
    profiling['5_6_render_maps'] = time.time() - step_start
    print(f"    âœ“ ì™„ë£Œ: {profiling['5_6_render_maps']:.2f}ì´ˆ")
    
    # ...
```

**ì˜ˆìƒ íš¨ê³¼:**
- ë Œë”ë§ ì‹œê°„: **10-15% ë‹¨ì¶•**
- ìºì‹œ íš¨ìœ¨ ì¦ê°€

---

### 5. **ë©”ëª¨ë¦¬ ìµœì í™”: Gradient Checkpointing** â­

#### í•´ê²°ì±…: ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¦ê°€

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        self.pipeline = pipeline.to(self.device)
        
        # âœ… Gradient Checkpointing í™œì„±í™” (ì¶”ë¡  ì‹œì—ë„ ë©”ëª¨ë¦¬ ì ˆì•½)
        if hasattr(self.pipeline.unet, 'enable_gradient_checkpointing'):
            self.pipeline.unet.enable_gradient_checkpointing()
        
        # âœ… xFormers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜
        if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
                print("    âœ“ xFormers í™œì„±í™”ë¨ (ë©”ëª¨ë¦¬ íš¨ìœ¨ ì¦ê°€)")
            except Exception:
                print("    âš  xFormers ë¯¸ì„¤ì¹˜ (ì„¤ì¹˜ ê¶Œì¥: pip install xformers)")
```

```python
# hy3dgen/texgen/utils/dehighlight_utils.pyì—ë„ ë™ì¼í•˜ê²Œ ì ìš©

class Light_Shadow_Remover():
    def __init__(self, config):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        self.pipeline = pipeline.to(self.device, torch.float16)
        
        # âœ… xFormers í™œì„±í™”
        if hasattr(self.pipeline, 'enable_xformers_memory_efficient_attention'):
            try:
                self.pipeline.enable_xformers_memory_efficient_attention()
            except Exception:
                pass
```

**ì˜ˆìƒ íš¨ê³¼:**
- GPU ë©”ëª¨ë¦¬: **20-30% ì ˆì•½**
- ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê°€ëŠ¥: batch_size 2 â†’ 3
- ê°„ì ‘ì  ì†ë„ í–¥ìƒ: **15-20%**

---

### 6. **ìºì‹± ì „ëµ** â­â­

#### í•´ê²°ì±…: ì¤‘ê°„ ê²°ê³¼ë¬¼ ìºì‹±

```python
# color.pyì— ìºì‹± ë¡œì§ ì¶”ê°€

import hashlib
import pickle

def get_image_hash(image_path):
    """ì´ë¯¸ì§€ íŒŒì¼ì˜ í•´ì‹œê°’ ê³„ì‚°"""
    with open(image_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
CACHE_DIR = 'my/cache'
os.makedirs(CACHE_DIR, exist_ok=True)

# í˜•ìƒ ìƒì„± ì „ì— ìºì‹œ í™•ì¸
image_hash = get_image_hash(INPUT_IMAGE)
cache_key = f"{image_hash}_{OCTREE_RESOLUTION}_{NUM_INFERENCE_STEPS}"
cache_path = os.path.join(CACHE_DIR, f"{cache_key}_mesh.pkl")

if os.path.exists(cache_path) and USE_CACHE:  # âœ… ìƒˆ ì„¤ì •
    print("=" * 60)
    print("ìºì‹œëœ í˜•ìƒì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
    with open(cache_path, 'rb') as f:
        mesh = pickle.load(f)
    print("ì™„ë£Œ!")
    print("=" * 60)
else:
    # ê¸°ì¡´ í˜•ìƒ ìƒì„± ì½”ë“œ
    mesh = shape_pipeline(...)
    
    # âœ… ìºì‹œ ì €ì¥
    if USE_CACHE:
        with open(cache_path, 'wb') as f:
            pickle.dump(mesh, f)
```

```python
# color.py ìƒë‹¨ì— ì„¤ì • ì¶”ê°€
USE_CACHE = True  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€
```

**ì˜ˆìƒ íš¨ê³¼:**
- ë™ì¼ ì´ë¯¸ì§€ ì¬ì²˜ë¦¬ ì‹œ: **í˜•ìƒ ìƒì„± ìŠ¤í‚µ** (~10ì´ˆ ì ˆì•½)
- íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹¤í—˜ ì‹œ ìœ ìš©

---

### 7. **UV ë˜í•‘ ìµœì í™”** â­

#### í•´ê²°ì±…: xatlas ë©€í‹°ìŠ¤ë ˆë”©

```python
# hy3dgen/texgen/utils/uv_warp_utils.py ìˆ˜ì •

import xatlas
import numpy as np

def mesh_uv_wrap(mesh, chart_options=None, pack_options=None):
    """
    xatlasë¥¼ ì‚¬ìš©í•œ UV ë˜í•‘ (ìµœì í™” ë²„ì „)
    """
    if chart_options is None:
        chart_options = xatlas.ChartOptions()
        chart_options.max_iterations = 1  # âœ… ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ (4 â†’ 1)
    
    if pack_options is None:
        pack_options = xatlas.PackOptions()
        pack_options.resolution = 2048  # âœ… í•´ìƒë„ ê³ ì •
        pack_options.padding = 2  # âœ… íŒ¨ë”© ê°ì†Œ (4 â†’ 2)
        pack_options.bruteForce = False  # âœ… ë¹ ë¥¸ íŒ¨í‚¹
        pack_options.maxChartSize = 0  # ì œí•œ ì—†ìŒ
        
    # xatlas ì‹¤í–‰
    vmapping, indices, uvs = xatlas.parametrize(
        mesh.vertices, 
        mesh.faces,
        chart_options=chart_options,
        pack_options=pack_options
    )
    
    # ë©”ì‰¬ ì—…ë°ì´íŠ¸
    vertices = mesh.vertices[vmapping]
    mesh_uv = mesh.__class__(vertices=vertices, faces=indices)
    mesh_uv.visual.uv = uvs
    
    return mesh_uv
```

**ì˜ˆìƒ íš¨ê³¼:**
- UV ë˜í•‘ ì‹œê°„: **30-50% ë‹¨ì¶•**
- 192 í•´ìƒë„: 22ì´ˆ â†’ 11-15ì´ˆ
- í’ˆì§ˆ ì €í•˜: ë¯¸ë¯¸ (í…ìŠ¤ì²˜ í’ˆì§ˆì€ ìœ ì§€)

---

### 8. **ì¸í˜ì¸íŒ… ìµœì í™”** â­

#### í•´ê²°ì±…: ë§ˆìŠ¤í¬ ì˜ì—­ ìµœì†Œí™”

```python
# hy3dgen/texgen/pipelines.pyì˜ í…ìŠ¤ì²˜ ì¸í˜ì¸íŒ… ì „ì²˜ë¦¬

def optimize_inpaint_mask(self, mask_np, texture):
    """
    ì¸í˜ì¸íŒ… ë§ˆìŠ¤í¬ ìµœì í™”: ì‘ì€ êµ¬ë©ì€ ë‹¨ìˆœ í™•ì¥ìœ¼ë¡œ ì±„ìš°ê¸°
    """
    import cv2
    
    # âœ… ì‘ì€ êµ¬ë© ì°¾ê¸°
    _, labels, stats, _ = cv2.connectedComponentsWithStats(255 - mask_np)
    
    small_hole_threshold = 100  # 100í”½ì…€ ì´í•˜ëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬
    for i in range(1, len(stats)):
        area = stats[i, cv2.CC_STAT_AREA]
        if area < small_hole_threshold:
            # ì‘ì€ êµ¬ë©ì€ ì£¼ë³€ ìƒ‰ìƒìœ¼ë¡œ í™•ì¥
            mask_np[labels == i] = 255  # ì¸í˜ì¸íŒ… ëŒ€ìƒì—ì„œ ì œì™¸
            
            # ê°„ë‹¨í•œ inpainting (ë¹ ë¦„)
            kernel = np.ones((5,5), np.uint8)
            texture_np = (texture.cpu().numpy() * 255).astype(np.uint8)
            texture_np = cv2.inpaint(texture_np, 255 - mask_np, 3, cv2.INPAINT_TELEA)
            texture = torch.tensor(texture_np / 255.0).to(texture.device)
    
    return texture, mask_np

# __call__ ë©”ì†Œë“œì—ì„œ ì‚¬ìš©
def __call__(self, mesh, image):
    # ...
    
    # 10. í…ìŠ¤ì²˜ ì¸í˜ì¸íŒ… ì „ì— ìµœì í™”
    step_start = time.time()
    print("    â†’ [10/11] í…ìŠ¤ì²˜ ì¸í˜ì¸íŒ… ì¤‘...")
    mask_np = (mask.squeeze(-1).cpu().numpy() * 255).astype(np.uint8)
    
    # âœ… ì‘ì€ êµ¬ë© ë¨¼ì € ì²˜ë¦¬
    texture, mask_np = self.optimize_inpaint_mask(mask_np, texture)
    
    # í° ì˜ì—­ë§Œ ê³ í’ˆì§ˆ ì¸í˜ì¸íŒ…
    texture = self.texture_inpaint(texture, mask_np)
    profiling['10_texture_inpaint'] = time.time() - step_start
    print(f"    âœ“ ì™„ë£Œ: {profiling['10_texture_inpaint']:.2f}ì´ˆ")
```

**ì˜ˆìƒ íš¨ê³¼:**
- ì¸í˜ì¸íŒ… ì‹œê°„: **20-40% ë‹¨ì¶•**
- 31-366ì´ˆ â†’ 20-220ì´ˆ

---

## ğŸ“‹ í†µí•© ì ìš© ê°€ì´ë“œ

### Step 1: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install xformers  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì–´í…ì…˜
pip install opencv-python  # ì¸í˜ì¸íŒ… ìµœì í™”
```

### Step 2: color.py ì„¤ì • ì—…ë°ì´íŠ¸

```python
# color.py ìƒë‹¨ì— ì¶”ê°€
USE_CACHE = True                    # ìºì‹œ í™œì„±í™”
ENABLE_TORCH_COMPILE = True         # Torch Compile í™œì„±í™”
BATCH_SIZE = 2                      # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ 1-3)
```

### Step 3: CustomTexGenConfig ìˆ˜ì •

```python
class CustomTexGenConfig:
    def __init__(self, light_remover_ckpt_path, multiview_ckpt_path, subfolder_name):
        self.device = 'cuda'
        self.light_remover_ckpt_path = light_remover_ckpt_path
        self.multiview_ckpt_path = multiview_ckpt_path
        self.candidate_camera_azims = camera_azims
        self.candidate_camera_elevs = camera_elevs
        self.candidate_view_weights = view_weights
        self.render_size = RENDER_SIZE
        self.texture_size = TEXTURE_SIZE
        self.delight_inference_steps = DELIGHT_INFERENCE_STEPS
        self.multiview_inference_steps = MULTIVIEW_INFERENCE_STEPS
        self.batch_size = BATCH_SIZE  # âœ… ì¶”ê°€
        self.bake_exp = 4
        self.merge_method = 'fast'
        self.pipe_dict = {'hunyuan3d-paint-v2-0': 'hunyuanpaint', 'hunyuan3d-paint-v2-0-turbo': 'hunyuanpaint-turbo'}
        self.pipe_name = self.pipe_dict[subfolder_name]
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

### í˜„ì¬ (ê¸°ì¤€ì„ )
- **ì´ ì‹œê°„**: ~470ì´ˆ (192 í•´ìƒë„, 6ë·°, standard ì„¤ì •)
- **í…ìŠ¤ì²˜ ìƒì„±**: ~420ì´ˆ (90.5%)
  - Multiview: ~315ì´ˆ (74.0%)
  - ì¸í˜ì¸íŒ…: ~31ì´ˆ (7.3%)
  - UV ë˜í•‘: ~22ì´ˆ (5.2%)
  - Delight: ~19ì´ˆ (4.4%)

### ìµœì í™” í›„ (ëª¨ë“  ë°©ì•ˆ ì ìš©)

| ë‹¨ê³„ | í˜„ì¬ | ìµœì í™” í›„ | ê°œì„ ìœ¨ |
|------|------|-----------|--------|
| Multiview | 315ì´ˆ | **190ì´ˆ** | **40% ë‹¨ì¶•** |
| ì¸í˜ì¸íŒ… | 31ì´ˆ | **20ì´ˆ** | **35% ë‹¨ì¶•** |
| UV ë˜í•‘ | 22ì´ˆ | **12ì´ˆ** | **45% ë‹¨ì¶•** |
| Delight | 19ì´ˆ | **13ì´ˆ** | **32% ë‹¨ì¶•** |
| **ì´í•©** | **470ì´ˆ** | **~280ì´ˆ** | **40% ë‹¨ì¶•** |

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. Torch Compile
- **ì²« ì‹¤í–‰**: +30ì´ˆ ì»´íŒŒì¼ ì˜¤ë²„í—¤ë“œ
- **ì´í›„ ì‹¤í–‰**: í° ì†ë„ í–¥ìƒ
- **í˜¸í™˜ì„±**: PyTorch 2.0+ í•„ìš”

### 2. ë°°ì¹˜ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬**: `batch_size=3`ì€ 16GB+ VRAM í•„ìš”
- **ê¶Œì¥**: RTX 3080 (10GB) â†’ batch_size=2
- **ì¡°ì •**: OOM ë°œìƒ ì‹œ 1ë¡œ ê°ì†Œ

### 3. xFormers
- **ì„¤ì¹˜**: CUDA ë²„ì „ê³¼ í˜¸í™˜ í•„ìˆ˜
- **ì—†ì–´ë„ ë¨**: ì„¤ì¹˜ ì‹¤íŒ¨í•´ë„ ë™ì‘ (ë‹¨, ëŠë¦¼)

### 4. UV ë˜í•‘ ìµœì í™”
- **í’ˆì§ˆ**: ì•½ê°„ì˜ UV ì”¸ ê°€ëŠ¥ì„±
- **ê¶Œì¥**: ê³ í’ˆì§ˆ í•„ìš” ì‹œ ì›ë³¸ ì„¤ì • ìœ ì§€

### 5. ìºì‹±
- **ë””ìŠ¤í¬ ê³µê°„**: ë©”ì‰¬ë‹¹ ~50-200MB
- **ê´€ë¦¬**: ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ í´ë” ì •ë¦¬

---

## ğŸ”§ ë‹¨ê³„ë³„ ì ìš© ìš°ì„ ìˆœìœ„

### ğŸ¥‡ 1ë‹¨ê³„ (ì¦‰ì‹œ ì ìš©, ìœ„í—˜ ë‚®ìŒ)
1. âœ… **xFormers ì„¤ì¹˜ ë° í™œì„±í™”** (ë°©ì•ˆ 5)
2. âœ… **Mixed Precision (autocast)** (ë°©ì•ˆ 3)
3. âœ… **ë Œë”ë§ ë³‘ë ¬í™”** (ë°©ì•ˆ 4)

**ì˜ˆìƒ íš¨ê³¼**: ~15-20% ì†ë„ í–¥ìƒ, ë©”ëª¨ë¦¬ 10-15% ì ˆì•½

### ğŸ¥ˆ 2ë‹¨ê³„ (ì•ˆì •ì , í…ŒìŠ¤íŠ¸ í•„ìš”)
4. âœ… **ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”** (ë°©ì•ˆ 1)
5. âœ… **UV ë˜í•‘ ìµœì í™”** (ë°©ì•ˆ 7)
6. âœ… **ìºì‹± ì „ëµ** (ë°©ì•ˆ 6)

**ì˜ˆìƒ íš¨ê³¼**: ì¶”ê°€ 20-25% ì†ë„ í–¥ìƒ

### ğŸ¥‰ 3ë‹¨ê³„ (ê³ ê¸‰, í™˜ê²½ ì˜ì¡´ì )
7. âœ… **Torch Compile** (ë°©ì•ˆ 2)
8. âœ… **ì¸í˜ì¸íŒ… ìµœì í™”** (ë°©ì•ˆ 8)

**ì˜ˆìƒ íš¨ê³¼**: ì¶”ê°€ 10-15% ì†ë„ í–¥ìƒ

---

## ğŸ’¡ ì¶”ê°€ ê³ ë ¤ì‚¬í•­

### A. í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ
- **GPU**: RTX 4090 (24GB) â†’ batch_size=4 ê°€ëŠ¥
- **íš¨ê³¼**: ë°°ì¹˜ í¬ê¸° ì¦ê°€ë¡œ **50%+ ì†ë„ í–¥ìƒ**

### B. ë¶„ì‚° ì²˜ë¦¬ (ë‹¤ì¤‘ GPU)
```python
# ê³ ê¸‰: ì—¬ëŸ¬ GPUë¡œ ë·° ë¶„ì‚° ì²˜ë¦¬
# êµ¬í˜„ ë³µì¡ë„: ë†’ìŒ
# íš¨ê³¼: GPU ê°œìˆ˜ë§Œí¼ ì„ í˜• ê°€ì†
```

### C. ëª¨ë¸ ì–‘ìí™” (INT8)
```python
# PyTorch ì–‘ìí™” (ì •í™•ë„ ì•½ê°„ ê°ì†Œ)
# ë©”ëª¨ë¦¬: 50% ì ˆì•½
# ì†ë„: 30-40% í–¥ìƒ
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

ìµœì í™” ì ìš© ì „ì— í™•ì¸:

- [ ] PyTorch 2.0+ ì„¤ì¹˜ í™•ì¸
- [ ] CUDA 11.7+ ì„¤ì¹˜ í™•ì¸
- [ ] GPU VRAM í¬ê¸° í™•ì¸ (ë°°ì¹˜ í¬ê¸° ê²°ì •)
- [ ] xFormers ì„¤ì¹˜ (ì„ íƒ)
- [ ] í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ë¡œ ë¨¼ì € ê²€ì¦
- [ ] ìºì‹œ ë””ë ‰í† ë¦¬ ìš©ëŸ‰ í™•ì¸
- [ ] ë°±ì—… ì™„ë£Œ

---

## ğŸ¯ ê²°ë¡ 

**í•µì‹¬ ë³‘ëª©**: Multiview ëª¨ë¸ (74%)
**ìµœê³  íš¨ê³¼**: ë°°ì¹˜ ì²˜ë¦¬ + xFormers + Torch Compile
**ì´ ê°œì„ **: **40% ì†ë„ í–¥ìƒ** (470ì´ˆ â†’ 280ì´ˆ)

**ê¶Œì¥ ì¡°í•©**:
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: 1-3ë‹¨ê³„ë§Œ ì ìš©
- í”„ë¡œë•ì…˜: ëª¨ë“  ë‹¨ê³„ ì ìš©
- í’ˆì§ˆ ìš°ì„ : Torch Compile ì œì™¸ (ì•ˆì •ì„±)

---

**ì‘ì„±ì¼**: 2025-11-02
**ë²„ì „**: Hunyuan3D-2 ìµœì í™” ê°€ì´ë“œ v1.0
