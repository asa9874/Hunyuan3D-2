# 8GB VRAM í™˜ê²½ í…ìŠ¤ì²˜ ë³‘ëª© í˜„ìƒ ìµœì í™” ë°©ì•ˆ

## ğŸ¯ ëª©í‘œ
**VRAM 8GB (RTX 3060, RTX 2070 ë“±)ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰í•˜ë©´ì„œ ì†ë„ ìµœì í™”**

---

## ğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš© ë¶„ì„

### ë©”ëª¨ë¦¬ í”„ë¡œí•„ (ê¸°ë³¸ ì„¤ì •)
```
íŒŒì´í”„ë¼ì¸ ë¡œë“œ:
  - Shape Pipeline: ~3-4 GB
  - Paint Pipeline ë¡œë“œ: ~7-8 GB (ì´ 10-12GB í•„ìš”)
  - í…ìŠ¤ì²˜ ìƒì„± í”¼í¬: ~11-12 GB âŒ 8GB ì´ˆê³¼!
```

### ì£¼ìš” ë©”ëª¨ë¦¬ ì†Œë¹„ ìš”ì†Œ
1. **Delight Model (Stable Diffusion)**: ~4 GB
2. **Multiview Model**: ~3-4 GB
3. **ë Œë”ëŸ¬ ë²„í¼**: ~1 GB
4. **ì„ì‹œ í…ìŠ¤ì²˜/ì¤‘ê°„ ê²°ê³¼ë¬¼**: ~2-3 GB

---

## ğŸš€ 8GB VRAM ìµœì í™” ë°©ì•ˆ

### 1. **Sequential Model Loading (ìˆœì°¨ì  ëª¨ë¸ ë¡œë“œ)** â­â­â­

#### ë¬¸ì œì 
í˜„ì¬ëŠ” Delightì™€ Multiview ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë™ì‹œì— ìƒì£¼í•©ë‹ˆë‹¤.

#### í•´ê²°ì±…: ì‚¬ìš© í›„ ì¦‰ì‹œ ì–¸ë¡œë“œ

```python
# hy3dgen/texgen/pipelines.py ìˆ˜ì •

class Hunyuan3DPaintPipeline:
    
    def __init__(self, config):
        self.config = config
        self.models = {}
        self.render = MeshRender(
            default_resolution=self.config.render_size,
            texture_size=self.config.texture_size)
        
        # âœ… ì´ˆê¸°ì—ëŠ” ëª¨ë¸ ë¡œë“œí•˜ì§€ ì•ŠìŒ
        self.delight_loaded = False
        self.multiview_loaded = False

    def load_delight_model(self):
        """Delight ëª¨ë¸ë§Œ ë¡œë“œ"""
        if not self.delight_loaded:
            print("    â†’ Delight ëª¨ë¸ ë¡œë“œ ì¤‘...")
            torch.cuda.empty_cache()
            self.models['delight_model'] = Light_Shadow_Remover(self.config)
            self.delight_loaded = True
            print(f"    âœ“ ë¡œë“œ ì™„ë£Œ (VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB)")

    def unload_delight_model(self):
        """Delight ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.delight_loaded:
            print("    â†’ Delight ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            del self.models['delight_model']
            self.delight_loaded = False
            torch.cuda.empty_cache()
            print(f"    âœ“ ì–¸ë¡œë“œ ì™„ë£Œ (VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB)")

    def load_multiview_model(self):
        """Multiview ëª¨ë¸ë§Œ ë¡œë“œ"""
        if not self.multiview_loaded:
            print("    â†’ Multiview ëª¨ë¸ ë¡œë“œ ì¤‘...")
            torch.cuda.empty_cache()
            self.models['multiview_model'] = Multiview_Diffusion_Net(self.config)
            self.multiview_loaded = True
            print(f"    âœ“ ë¡œë“œ ì™„ë£Œ (VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB)")

    def unload_multiview_model(self):
        """Multiview ëª¨ë¸ ì–¸ë¡œë“œ"""
        if self.multiview_loaded:
            print("    â†’ Multiview ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            del self.models['multiview_model']
            self.multiview_loaded = False
            torch.cuda.empty_cache()
            print(f"    âœ“ ì–¸ë¡œë“œ ì™„ë£Œ (VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB)")

    @torch.no_grad()
    def __call__(self, mesh, image):
        import time
        profiling = {}
        total_start = time.time()

        if not isinstance(image, List):
            image = [image]

        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        step_start = time.time()
        print("    â†’ [1/11] ì´ë¯¸ì§€ ì¤‘ì•™ ì •ë ¬ ì¤‘...")
        images_prompt = []
        for i in range(len(image)):
            if isinstance(image[i], str):
                image_prompt = Image.open(image[i])
            else:
                image_prompt = image[i]
            images_prompt.append(image_prompt)
        images_prompt = [self.recenter_image(img) for img in images_prompt]
        profiling['1_image_recenter'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['1_image_recenter']:.2f}ì´ˆ")

        # âœ… 2. Delight ëª¨ë¸ (ì‚¬ìš© í›„ ì¦‰ì‹œ ì–¸ë¡œë“œ)
        step_start = time.time()
        print("    â†’ [2/11] Delight ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        self.load_delight_model()
        images_prompt = [self.models['delight_model'](img) for img in images_prompt]
        self.unload_delight_model()  # âœ… ì¦‰ì‹œ ì–¸ë¡œë“œ
        profiling['2_delight_model'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['2_delight_model']:.2f}ì´ˆ")

        # 3-6. UV, ë Œë”ë§ (ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©)
        step_start = time.time()
        print("    â†’ [3/11] UV Wrapping ì¤‘...")
        mesh = mesh_uv_wrap(mesh)
        profiling['3_uv_wrap'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['3_uv_wrap']:.2f}ì´ˆ")

        step_start = time.time()
        print("    â†’ [4/11] ë©”ì‰¬ ë¡œë“œ ì¤‘...")
        self.render.load_mesh(mesh)
        profiling['4_mesh_load'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['4_mesh_load']:.2f}ì´ˆ")

        selected_camera_elevs = self.config.candidate_camera_elevs
        selected_camera_azims = self.config.candidate_camera_azims
        selected_view_weights = self.config.candidate_view_weights

        step_start = time.time()
        print(f"    â†’ [5/11] Normal ë§µ ë Œë”ë§ ì¤‘...")
        normal_maps = self.render_normal_multiview(
            selected_camera_elevs, selected_camera_azims, use_abs_coor=True)
        profiling['5_render_normal'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['5_render_normal']:.2f}ì´ˆ")

        step_start = time.time()
        print(f"    â†’ [6/11] Position ë§µ ë Œë”ë§ ì¤‘...")
        position_maps = self.render_position_multiview(
            selected_camera_elevs, selected_camera_azims)
        profiling['6_render_position'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['6_render_position']:.2f}ì´ˆ")

        # âœ… 7. Multiview ìƒì„± (ë¡œë“œ â†’ ì‚¬ìš© â†’ ì–¸ë¡œë“œ)
        step_start = time.time()
        print("    â†’ [7/11] Multiview ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
        self.load_multiview_model()
        
        camera_info = [(((azim // 30) + 9) % 12) // {-20: 1, 0: 1, 20: 1, -90: 3, 90: 3}[
            elev] + {-20: 0, 0: 12, 20: 24, -90: 36, 90: 40}[elev] 
            for azim, elev in zip(selected_camera_azims, selected_camera_elevs)]
        
        multiviews = self.models['multiview_model'](
            images_prompt, normal_maps + position_maps, camera_info)
        
        self.unload_multiview_model()  # âœ… ì¦‰ì‹œ ì–¸ë¡œë“œ
        profiling['7_multiview_model'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['7_multiview_model']:.2f}ì´ˆ")

        # 8-11. ë‚˜ë¨¸ì§€ ë‹¨ê³„ë“¤
        step_start = time.time()
        print("    â†’ [8/11] ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ì¤‘...")
        for i in range(len(multiviews)):
            multiviews[i] = multiviews[i].resize(
                (self.config.render_size, self.config.render_size))
        profiling['8_image_resize'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['8_image_resize']:.2f}ì´ˆ")

        step_start = time.time()
        print("    â†’ [9/11] í…ìŠ¤ì²˜ ë² ì´í‚¹ ì¤‘...")
        texture, mask = self.bake_from_multiview(
            multiviews, selected_camera_elevs, selected_camera_azims, 
            selected_view_weights, method=self.config.merge_method)
        profiling['9_texture_bake'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['9_texture_bake']:.2f}ì´ˆ")

        step_start = time.time()
        print("    â†’ [10/11] í…ìŠ¤ì²˜ ì¸í˜ì¸íŒ… ì¤‘...")
        mask_np = (mask.squeeze(-1).cpu().numpy() * 255).astype(np.uint8)
        texture = self.texture_inpaint(texture, mask_np)
        profiling['10_texture_inpaint'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['10_texture_inpaint']:.2f}ì´ˆ")

        step_start = time.time()
        print("    â†’ [11/11] ìµœì¢… ë©”ì‰¬ ì €ì¥ ì¤‘...")
        self.render.set_texture(texture)
        textured_mesh = self.render.save_mesh()
        profiling['11_mesh_save'] = time.time() - step_start
        print(f"    âœ“ ì™„ë£Œ: {profiling['11_mesh_save']:.2f}ì´ˆ")

        profiling['TOTAL'] = time.time() - total_start
        self.profiling = profiling

        # í”„ë¡œíŒŒì¼ë§ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ” í…ìŠ¤ì²˜ ìƒì„± ë‹¨ê³„ë³„ ì‹œê°„ ë¶„ì„")
        print("="*60)
        for step, elapsed in profiling.items():
            if step != 'TOTAL':
                percentage = (elapsed / profiling['TOTAL']) * 100
                print(f"  {step:25s}: {elapsed:6.2f}ì´ˆ ({percentage:5.1f}%)")
        print("-"*60)
        print(f"  {'TOTAL':25s}: {profiling['TOTAL']:6.2f}ì´ˆ")
        print("="*60 + "\n")

        return textured_mesh
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 4-5 GB â†’ **ìµœëŒ€ ì‚¬ìš©ëŸ‰ 7-8GB ì´ë‚´**

---

### 2. **ì €í•´ìƒë„ ì„¤ì • (8GB ìµœì í™”)** â­â­â­

#### color.py ê¶Œì¥ ì„¤ì •

```python
# ==== 8GB VRAM ìµœì í™” ì„¤ì • ====

# ê²½ë¡œ ì„¤ì •
INPUT_IMAGE = 'my/input/bag.jpg'

# ë°°ê²½ ì œê±°
REMOVE_BACKGROUND = True

# í˜•ìƒ ìƒì„± (ë©”ëª¨ë¦¬ ì ˆì•½)
NUM_INFERENCE_STEPS = 4             # 5 â†’ 4 (ì•½ê°„ ë¹ ë¥´ê²Œ)
OCTREE_RESOLUTION = 128             # 192 â†’ 128 (ë©”ëª¨ë¦¬ ì ˆì•½) âœ…
GUIDANCE_SCALE = 5

# í…ìŠ¤ì²˜ ìƒì„± (ëŒ€í­ ì ˆì•½)
DELIGHT_INFERENCE_STEPS = 5         # 6 â†’ 5
MULTIVIEW_INFERENCE_STEPS = 5       # 6 â†’ 5

# ì¹´ë©”ë¼ ë·° (ì†ë„+ë©”ëª¨ë¦¬ ì ˆì•½)
CAMERA_VIEWS = 'fast'               # 6ë·° â†’ 4ë·° âœ…

# ë Œë”ë§ (ë©”ëª¨ë¦¬ ì ˆì•½)
RENDER_SIZE = 1536                  # 2048 â†’ 1536 âœ…
TEXTURE_SIZE = 1536                 # 2048 â†’ 1536 âœ…
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 
- OCTREE 128: -1 GB
- RENDER_SIZE 1536: -0.5 GB
- TEXTURE_SIZE 1536: -0.5 GB
- 4ë·°: -1 GB
**ì´ ì ˆì•½**: ~3 GB

---

### 3. **VAE Tiling & Slicing (í•„ìˆ˜)** â­â­â­

#### Delight ë° Multiview ëª¨ë¸ì— ì ìš©

```python
# hy3dgen/texgen/utils/dehighlight_utils.py

class Light_Shadow_Remover():
    def __init__(self, config):
        self.device = config.device
        self.cfg_image = 1.5
        self.cfg_text = 1.0
        self.num_inference_steps = getattr(config, 'delight_inference_steps', 6)

        pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            config.light_remover_ckpt_path,
            torch_dtype=torch.float16,
            safety_checker=None,
        )
        pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(
            pipeline.scheduler.config)

        # âœ… VAE ë©”ëª¨ë¦¬ ìµœì í™” (8GB í•„ìˆ˜!)
        if hasattr(pipeline, 'vae'):
            pipeline.vae.enable_tiling()        # íƒ€ì¼ ë°©ì‹ ì²˜ë¦¬
            pipeline.vae.enable_slicing()       # ìŠ¬ë¼ì´ìŠ¤ ì²˜ë¦¬
            print("    âœ“ VAE Tiling/Slicing í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)")

        self.pipeline = pipeline.to(self.device, torch.float16)
```

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        pipeline = DiffusionPipeline.from_pretrained(...)
        
        # âœ… VAE ë©”ëª¨ë¦¬ ìµœì í™”
        if hasattr(pipeline, 'vae'):
            pipeline.vae.enable_tiling()
            pipeline.vae.enable_slicing()
            print("    âœ“ VAE Tiling/Slicing í™œì„±í™”")

        self.pipeline = pipeline.to(self.device)
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 1-2 GB

---

### 4. **Attention Slicing (ë©”ëª¨ë¦¬ ëŒ€í­ ì ˆì•½)** â­â­â­

```python
# hy3dgen/texgen/utils/dehighlight_utils.py

class Light_Shadow_Remover():
    def __init__(self, config):
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # âœ… Attention Slicing (8GB í•„ìˆ˜!)
        self.pipeline.enable_attention_slicing(slice_size='auto')
        
        # âœ… VAE ìµœì í™”
        if hasattr(self.pipeline, 'vae'):
            self.pipeline.vae.enable_tiling()
            self.pipeline.vae.enable_slicing()

        self.pipeline = pipeline.to(self.device, torch.float16)
```

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __init__(self, config) -> None:
        # ... ê¸°ì¡´ ì½”ë“œ ...
        
        # âœ… Attention Slicing
        self.pipeline.enable_attention_slicing(slice_size='auto')
        
        # âœ… VAE ìµœì í™”
        if hasattr(pipeline, 'vae'):
            pipeline.vae.enable_tiling()
            pipeline.vae.enable_slicing()

        self.pipeline = pipeline.to(self.device)
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 2-3 GB (ê°€ì¥ íš¨ê³¼ì !)

---

### 5. **CPU Offloading (ëŠë¦¬ì§€ë§Œ ì•ˆì „)** â­

#### ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ëª¨ë¸ì„ CPUë¡œ ì¼ë¶€ ì´ë™

```python
# hy3dgen/texgen/pipelines.py

class Hunyuan3DPaintPipeline:
    
    def enable_model_cpu_offload(self, gpu_id: Optional[int] = None, 
                                  device: Union[torch.device, str] = "cuda"):
        """
        âœ… 8GB VRAMìš©: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë¥¼ CPUë¡œ ì´ë™
        ì†ë„ëŠ” ëŠë ¤ì§€ì§€ë§Œ ë©”ëª¨ë¦¬ ì•ˆì „
        """
        if hasattr(self.models.get('delight_model'), 'pipeline'):
            self.models['delight_model'].pipeline.enable_model_cpu_offload(
                gpu_id=gpu_id, device=device)
            print("    âœ“ Delight CPU Offload í™œì„±í™”")
        
        if hasattr(self.models.get('multiview_model'), 'pipeline'):
            self.models['multiview_model'].pipeline.enable_model_cpu_offload(
                gpu_id=gpu_id, device=device)
            print("    âœ“ Multiview CPU Offload í™œì„±í™”")
```

```python
# color.pyì—ì„œ ì‚¬ìš©
paint_pipeline = Hunyuan3DPaintPipeline.from_pretrained(...)

# âœ… 8GB VRAMì´ë©´ í™œì„±í™”
if torch.cuda.get_device_properties(0).total_memory < 10 * 1024**3:  # 10GB ë¯¸ë§Œ
    print("âš ï¸  VRAM ë¶€ì¡± ê°ì§€ (8GB ì´í•˜)")
    print("   â†’ CPU Offload í™œì„±í™” (ì†ë„ ëŠë¦¼, ë©”ëª¨ë¦¬ ì•ˆì „)")
    paint_pipeline.enable_model_cpu_offload()
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 3-4 GB
**ì†ë„ ì €í•˜**: ~30% ëŠë¦¼

---

### 6. **Gradient Accumulation ì—†ì´ ì²˜ë¦¬** â­â­

```python
# hy3dgen/texgen/utils/multiview_utils.py

class Multiview_Diffusion_Net():
    def __call__(self, input_images, control_images, camera_info):
        
        # âœ… ë·°ë¥¼ í•˜ë‚˜ì”© ìˆœì°¨ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì•ˆì „)
        num_view = len(control_images) // 2
        all_images = []
        
        for view_idx in range(num_view):
            print(f"      â†’ ë·° {view_idx+1}/{num_view} ì²˜ë¦¬ ì¤‘...")
            
            # ë‹¨ì¼ ë·°ë§Œ ì²˜ë¦¬
            normal_image = [[control_images[view_idx]]]
            position_image = [[control_images[view_idx + num_view]]]
            camera_info_single = [[camera_info[view_idx]]]
            
            kwargs = {
                'generator': torch.Generator(device=self.pipeline.device).manual_seed(0),
                'width': self.view_size,
                'height': self.view_size,
                'num_in_batch': 1,  # âœ… í•œ ë²ˆì— 1ê°œë§Œ
                'camera_info_gen': camera_info_single,
                'camera_info_ref': [[0]],
                'normal_imgs': normal_image,
                'position_imgs': position_image
            }

            with torch.cuda.amp.autocast(enabled=True, dtype=torch.float16):
                single_image = self.pipeline(
                    input_images, 
                    num_inference_steps=self.num_inference_steps, 
                    **kwargs
                ).images[0]
            
            all_images.append(single_image)
            
            # âœ… ë§¤ ë·°ë§ˆë‹¤ ìºì‹œ ë¹„ìš°ê¸°
            torch.cuda.empty_cache()
        
        return all_images
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆì•½**: 1-2 GB
**ì†ë„ ì €í•˜**: ì•½ê°„ ëŠë¦¼ (5-10%)

---

### 7. **Torch.no_grad() & ë©”ëª¨ë¦¬ ì •ë¦¬** â­â­

```python
# hy3dgen/texgen/pipelines.py

@torch.no_grad()  # âœ… ì´ë¯¸ ìˆìŒ
def __call__(self, mesh, image):
    
    # ê° ë‹¨ê³„ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
    step_start = time.time()
    print("    â†’ [2/11] Delight ëª¨ë¸ ì‹¤í–‰ ì¤‘...")
    images_prompt = [self.models['delight_model'](img) for img in images_prompt]
    profiling['2_delight_model'] = time.time() - step_start
    
    # âœ… ì¦‰ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()
    import gc
    gc.collect()
    
    print(f"    âœ“ ì™„ë£Œ: {profiling['2_delight_model']:.2f}ì´ˆ")
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨**: +10-15%

---

### 8. **í•´ìƒë„ ë™ì  ì¡°ì •** â­

```python
# color.py ìƒë‹¨ì— ì¶”ê°€

import torch

# âœ… VRAM ìë™ ê°ì§€ ë° ì„¤ì • ì¡°ì •
def get_optimal_settings_for_vram():
    """VRAM í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜"""
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA ì‚¬ìš© ë¶ˆê°€")
    
    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"\nğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š VRAM: {vram_gb:.1f} GB")
    
    if vram_gb < 10:  # 8GB
        print("âš ï¸  8GB VRAM ê°ì§€ â†’ ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ")
        return {
            'OCTREE_RESOLUTION': 128,
            'CAMERA_VIEWS': 'fast',
            'RENDER_SIZE': 1536,
            'TEXTURE_SIZE': 1536,
            'NUM_INFERENCE_STEPS': 4,
            'DELIGHT_INFERENCE_STEPS': 5,
            'MULTIVIEW_INFERENCE_STEPS': 5,
            'USE_CPU_OFFLOAD': True
        }
    elif vram_gb < 16:  # 12GB
        print("âœ… 12GB VRAM â†’ ê· í˜• ëª¨ë“œ")
        return {
            'OCTREE_RESOLUTION': 192,
            'CAMERA_VIEWS': 'fast',
            'RENDER_SIZE': 2048,
            'TEXTURE_SIZE': 2048,
            'NUM_INFERENCE_STEPS': 5,
            'DELIGHT_INFERENCE_STEPS': 6,
            'MULTIVIEW_INFERENCE_STEPS': 6,
            'USE_CPU_OFFLOAD': False
        }
    else:  # 16GB+
        print("ğŸš€ 16GB+ VRAM â†’ ê³ í’ˆì§ˆ ëª¨ë“œ")
        return {
            'OCTREE_RESOLUTION': 192,
            'CAMERA_VIEWS': 'standard',
            'RENDER_SIZE': 2048,
            'TEXTURE_SIZE': 2048,
            'NUM_INFERENCE_STEPS': 5,
            'DELIGHT_INFERENCE_STEPS': 6,
            'MULTIVIEW_INFERENCE_STEPS': 6,
            'USE_CPU_OFFLOAD': False
        }

# âœ… ìë™ ì„¤ì • ì ìš©
optimal_settings = get_optimal_settings_for_vram()

INPUT_IMAGE = 'my/input/bag.jpg'
REMOVE_BACKGROUND = True

# ìë™ ì¡°ì •ëœ ì„¤ì •
NUM_INFERENCE_STEPS = optimal_settings['NUM_INFERENCE_STEPS']
OCTREE_RESOLUTION = optimal_settings['OCTREE_RESOLUTION']
GUIDANCE_SCALE = 5
DELIGHT_INFERENCE_STEPS = optimal_settings['DELIGHT_INFERENCE_STEPS']
MULTIVIEW_INFERENCE_STEPS = optimal_settings['MULTIVIEW_INFERENCE_STEPS']
CAMERA_VIEWS = optimal_settings['CAMERA_VIEWS']
RENDER_SIZE = optimal_settings['RENDER_SIZE']
TEXTURE_SIZE = optimal_settings['TEXTURE_SIZE']
USE_CPU_OFFLOAD = optimal_settings['USE_CPU_OFFLOAD']
```

---

## ğŸ“‹ 8GB VRAM í†µí•© ì ìš© ê°€ì´ë“œ

### Step 1: pipelines.py ìˆ˜ì • (ìˆœì°¨ ë¡œë“œ/ì–¸ë¡œë“œ)

ìœ„ì˜ **ë°©ì•ˆ 1** ì½”ë“œë¥¼ `hy3dgen/texgen/pipelines.py`ì— ì ìš©

### Step 2: VAE & Attention ìµœì í™”

`dehighlight_utils.py`ì™€ `multiview_utils.py`ì— ë‹¤ìŒ ì¶”ê°€:
```python
# VAE ìµœì í™”
pipeline.vae.enable_tiling()
pipeline.vae.enable_slicing()

# Attention ìµœì í™”
pipeline.enable_attention_slicing(slice_size='auto')
```

### Step 3: color.py ì„¤ì • ë³€ê²½

```python
# 8GB ìµœì í™” ì„¤ì •
NUM_INFERENCE_STEPS = 4
OCTREE_RESOLUTION = 128
DELIGHT_INFERENCE_STEPS = 5
MULTIVIEW_INFERENCE_STEPS = 5
CAMERA_VIEWS = 'fast'
RENDER_SIZE = 1536
TEXTURE_SIZE = 1536
```

### Step 4: ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€

```python
# color.pyì— ì¶”ê°€

def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB")

# ê° ë‹¨ê³„ë§ˆë‹¤ í˜¸ì¶œ
print_gpu_memory()
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ (8GB VRAM)

### í˜„ì¬ (ì‹¤í–‰ ë¶ˆê°€)
- **ìµœëŒ€ ë©”ëª¨ë¦¬**: ~12 GB âŒ OOM ì—ëŸ¬

### ìµœì í™” í›„ (ì•ˆì • ì‹¤í–‰)
- **ìµœëŒ€ ë©”ëª¨ë¦¬**: ~7.5 GB âœ…
- **ì´ ì‹œê°„**: ~520ì´ˆ (ê¸°ì¡´ 470ì´ˆ ëŒ€ë¹„ +10%)
- **í’ˆì§ˆ**: ì•½ê°„ ì €í•˜ (128 í•´ìƒë„, 4ë·°)

### ì„¸ë¶€ ë©”ëª¨ë¦¬ ì ˆì•½

| ìµœì í™” ë°©ë²• | ë©”ëª¨ë¦¬ ì ˆì•½ | ì†ë„ ì˜í–¥ |
|------------|------------|----------|
| ìˆœì°¨ ë¡œë“œ/ì–¸ë¡œë“œ | -4 GB | +20ì´ˆ |
| VAE Tiling/Slicing | -2 GB | +5ì´ˆ |
| Attention Slicing | -2.5 GB | +10ì´ˆ |
| ì €í•´ìƒë„ (128/1536) | -3 GB | -50ì´ˆ |
| **ì´í•©** | **-11.5 GB** | **-15ì´ˆ** |

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
- **OCTREE 128**: ë””í…Œì¼ ê°ì†Œ (~30%)
- **RENDER_SIZE 1536**: í…ìŠ¤ì²˜ ì„ ëª…ë„ ì•½ê°„ ì €í•˜
- **4ë·°**: í›„ë©´/ì¸¡ë©´ í’ˆì§ˆ ì €í•˜

### 2. ì†ë„
- **ìˆœì°¨ ë¡œë“œ**: ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì‹œê°„ ì¶”ê°€ (+20ì´ˆ)
- **ë‹¨ì¼ ë·° ì²˜ë¦¬**: ë°°ì¹˜ íš¨ê³¼ ì—†ìŒ (+10ì´ˆ)
- **ì´ ì˜ˆìƒ ì‹œê°„**: ~520ì´ˆ (ê¸°ì¡´ ëŒ€ë¹„ +50ì´ˆ)

### 3. ì•ˆì •ì„±
- **ì²« ì‹¤í–‰**: ë©”ëª¨ë¦¬ ì •ë¦¬ë¥¼ ìœ„í•´ ì‹œìŠ¤í…œ ì¬ë¶€íŒ… ê¶Œì¥
- **ë‹¤ë¥¸ í”„ë¡œê·¸ë¨**: ì¢…ë£Œ (í¬ë¡¬, VSCode ë“±)
- **Windows**: ê°€ìƒ ë©”ëª¨ë¦¬ ì„¤ì • í™•ì¸

---

## ğŸ¯ 8GB VRAM ê¶Œì¥ ì›Œí¬í”Œë¡œìš°

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… (8GB ì•ˆì „)
```python
NUM_INFERENCE_STEPS = 3
OCTREE_RESOLUTION = 128
CAMERA_VIEWS = 'minimal'  # 3ë·°
RENDER_SIZE = 1024
TEXTURE_SIZE = 1024
```
â±ï¸ **ì‹œê°„**: ~350ì´ˆ
ğŸ’¾ **VRAM**: ~6.5 GB

### ì‹œë‚˜ë¦¬ì˜¤ 2: ê· í˜• (8GB ê¶Œì¥)
```python
NUM_INFERENCE_STEPS = 4
OCTREE_RESOLUTION = 128
CAMERA_VIEWS = 'fast'  # 4ë·°
RENDER_SIZE = 1536
TEXTURE_SIZE = 1536
```
â±ï¸ **ì‹œê°„**: ~520ì´ˆ
ğŸ’¾ **VRAM**: ~7.5 GB

### ì‹œë‚˜ë¦¬ì˜¤ 3: ìµœê³  í’ˆì§ˆ (8GB í•œê³„)
```python
NUM_INFERENCE_STEPS = 5
OCTREE_RESOLUTION = 192  # âš ï¸ ìœ„í—˜
CAMERA_VIEWS = 'fast'
RENDER_SIZE = 1536
TEXTURE_SIZE = 1536
USE_CPU_OFFLOAD = True  # í•„ìˆ˜!
```
â±ï¸ **ì‹œê°„**: ~650ì´ˆ
ğŸ’¾ **VRAM**: ~7.8 GB (ì•„ìŠ¬ì•„ìŠ¬)

---

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### OOM ì—ëŸ¬ ë°œìƒ ì‹œ
1. âœ… OCTREE_RESOLUTION = 128ë¡œ ê°ì†Œ
2. âœ… RENDER_SIZE = 1024ë¡œ ê°ì†Œ
3. âœ… CAMERA_VIEWS = 'minimal'ë¡œ ë³€ê²½
4. âœ… CPU Offload í™œì„±í™”
5. âœ… ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
6. âœ… ì‹œìŠ¤í…œ ì¬ë¶€íŒ…

### ë„ˆë¬´ ëŠë¦´ ê²½ìš°
- CPU Offload ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì£¼ì˜)
- CAMERA_VIEWS = 'minimal' (3ë·°)
- NUM_INFERENCE_STEPS = 3

### í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ì„ ê²½ìš°
- TEXTURE_SIZE = 2048 (ë©”ëª¨ë¦¬ ì£¼ì˜)
- CAMERA_VIEWS = 'fast' (4ë·°)
- OCTREE_RESOLUTION = 192 (ìœ„í—˜, CPU Offload í•„ìˆ˜)

---

## ğŸ’¡ ì¶”ê°€ íŒ

### 1. ìœˆë„ìš° ê°€ìƒ ë©”ëª¨ë¦¬ ì„¤ì •
```
ì‹œìŠ¤í…œ ì†ì„± â†’ ê³ ê¸‰ â†’ ì„±ëŠ¥ ì„¤ì • â†’ ê³ ê¸‰ â†’ ê°€ìƒ ë©”ëª¨ë¦¬
ìµœì†Œ: 16GB, ìµœëŒ€: 32GB
```

### 2. ëª¨ë‹ˆí„°ë§ ë„êµ¬
```bash
# NVIDIA GPU ëª¨ë‹ˆí„°ë§
nvidia-smi -l 1
```

### 3. ë°°ì¹˜ ì²˜ë¦¬
```python
# ì—¬ëŸ¬ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œ ê°ê° ë…ë¦½ ì‹¤í–‰
for image_path in image_list:
    # ê° ì´ë¯¸ì§€ë§ˆë‹¤ í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘ (ë©”ëª¨ë¦¬ ì™„ì „ ì •ë¦¬)
    subprocess.run(['python', 'color.py', image_path])
```

---

## ğŸ¯ ê²°ë¡ 

**8GB VRAMì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥!**

**í•µì‹¬ ì „ëµ**:
1. â­â­â­ **ìˆœì°¨ ë¡œë“œ/ì–¸ë¡œë“œ** (í•„ìˆ˜)
2. â­â­â­ **VAE + Attention Slicing** (í•„ìˆ˜)
3. â­â­â­ **ì €í•´ìƒë„ ì„¤ì •** (128/1536)
4. â­ CPU Offload (í•„ìš” ì‹œ)

**ì˜ˆìƒ ê²°ê³¼**:
- ë©”ëª¨ë¦¬: 12GB â†’ **7.5GB** âœ…
- ì‹œê°„: 470ì´ˆ â†’ **520ì´ˆ** (+10%)
- í’ˆì§ˆ: ì•½ê°„ ì €í•˜ (í”„ë¡œí† íƒ€ì… ì¶©ë¶„)

**ê¶Œì¥ GPU ì—…ê·¸ë ˆì´ë“œ**:
- RTX 3060 12GB
- RTX 4060 Ti 16GB
- RTX 4070 12GB

---

**ì‘ì„±ì¼**: 2025-11-02
**ë²„ì „**: 8GB VRAM ìµœì í™” ê°€ì´ë“œ v1.0
